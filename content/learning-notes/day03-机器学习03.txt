æœºå™¨å­¦ä¹ 03

day03-

ä¸Šè¯¾æ—¥æœŸ
@June 18, 2025
ä¸»è®²è€å¸ˆ
ææ™“å
å¤ä¹ 
@June 19, 2025
æœ¬è¯¾ç¨‹å…³é”®è¯ æœºå™¨å­¦ä¹ ï½œæ•°æ®é¢„å¤„ç†ï½œçº¿æ€§å›å½’ï½œé€»è¾‘å›å½’ï½œæ¨¡å‹è¯„ä¼°ï½œKNNç®—æ³•
è¯¾æ—¶
2

ğŸ“š ç›®å½•
ğŸ¯ æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ

æœ‰ç›‘ç£å­¦ä¹ æµç¨‹
å›å½’ vs åˆ†ç±»
ğŸ”§ æ•°æ®é¢„å¤„ç†ä¸æ ‡å‡†åŒ–
æ•°æ®æ ‡å‡†åŒ–çš„é‡è¦æ€§
æ ‡å‡†åŒ–å…¬å¼
ä»£ç å®ç°
ğŸ“ˆ çº¿æ€§å›å½’
ç†è®ºåŸºç¡€
æŸå¤±å‡½æ•°
å®Œæ•´ä»£ç ç¤ºä¾‹
ğŸ§  é€»è¾‘å›å½’ä¸åˆ†ç±»
Sigmoid æ¿€æ´»å‡½æ•°
Sigmoid å‡½æ•°ç‰¹æ€§
ä»£ç å®ç°
é€»è¾‘å›å½’å®Œæ•´æµç¨‹
ğŸ“Š æ¨¡å‹è¯„ä¼°
å›å½’é—®é¢˜è¯„ä¼°æŒ‡æ ‡
åˆ†ç±»é—®é¢˜è¯„ä¼°æŒ‡æ ‡
ğŸ’¾ æ¨¡å‹ä¿å­˜ä¸åŠ è½½
åºåˆ—åŒ–æ¦‚å¿µ
Python åºåˆ—åŒ–å·¥å…·å¯¹æ¯”
ä»£ç ç¤ºä¾‹
ğŸ“ ç»Ÿè®¡å­¦æ¦‚å¿µï¼šæ–¹å·®
æ–¹å·®çš„ä¸¤ç§ä¼°è®¡æ–¹æ³•
1. æ€»ä½“æ–¹å·®ï¼ˆæœ‰åä¼°è®¡ï¼‰
2. æ ·æœ¬æ–¹å·®ï¼ˆæ— åä¼°è®¡ï¼‰
åº“å‡½æ•°å·®å¼‚
ä»£ç ç¤ºä¾‹
âš ï¸ æ³¨æ„äº‹é¡¹
ğŸš€ KNNç®—æ³•è¡¥å……
Kè¿‘é‚»ç®—æ³•ç‰¹ç‚¹
ä»£ç ç¤ºä¾‹
ğŸ¯ å…³é”®è¦ç‚¹æ€»ç»“
âœ… æœ€ä½³å®è·µ
ğŸ” æ·±å…¥ç†è§£
ğŸ“ˆ è¿›é˜¶æ–¹å‘

ğŸ“š ç›®å½•

æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ
2. æ•°æ®é¢„å¤„ç†ä¸æ ‡å‡†åŒ–
3. çº¿æ€§å›å½’
4. é€»è¾‘å›å½’ä¸åˆ†ç±»
5. æ¨¡å‹è¯„ä¼°
6. æ¨¡å‹ä¿å­˜ä¸åŠ è½½
7. ç»Ÿè®¡å­¦æ¦‚å¿µï¼šæ–¹å·®
1.

æœºå™¨å­¦ä¹ 03

day03-

1

ğŸ¯ æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ
æœ‰ç›‘ç£å­¦ä¹ æµç¨‹

æœºå™¨å­¦ä¹ çš„æ ‡å‡†æµç¨‹åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªå…³é”®æ­¥éª¤ï¼š
1. æ•°æ®æ”¶é›†ä¸åŠ è½½
2. ç‰¹å¾å·¥ç¨‹ä¸æ•°æ®é¢„å¤„ç†
3. æ¨¡å‹é€‰æ‹©ä¸è®­ç»ƒ
4. æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–
5. æ¨¡å‹éƒ¨ç½²ä¸ä¿å­˜

å›å½’ vs åˆ†ç±»

å›å½’é—®é¢˜ï¼šé¢„æµ‹è¿ç»­æ•°å€¼ï¼ˆå¦‚æˆ¿ä»·é¢„æµ‹ï¼‰
åˆ†ç±»é—®é¢˜ï¼šé¢„æµ‹ç¦»æ•£ç±»åˆ«ï¼ˆå¦‚ç–¾ç—…è¯Šæ–­ï¼‰

ğŸ”§ æ•°æ®é¢„å¤„ç†ä¸æ ‡å‡†åŒ–
æ•°æ®æ ‡å‡†åŒ–çš„é‡è¦æ€§

æ•°æ®æ ‡å‡†åŒ–æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®æ­¥éª¤ï¼Œç›®çš„æ˜¯æ¶ˆé™¤ä¸åŒç‰¹å¾ä¹‹é—´é‡çº²å·®å¼‚çš„å½±å“ã€‚

æ ‡å‡†åŒ–å…¬å¼

X âˆ’ Î¼
Xnormalized =
â€‹

â€‹

Ïƒ + Ïµ

å…¶ä¸­ï¼š
Î¼ æ˜¯å‡å€¼
Ïƒ æ˜¯æ ‡å‡†å·®
Îµ = 1e-9 æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°

ä»£ç å®ç°

è®¡ç®—è®­ç»ƒé›†çš„å‡å€¼å’Œæ ‡å‡†å·®

#
mu = X_train.mean(axis=0)
sigma = X_train.std(axis=0) + 1e-9

å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œæ ‡å‡†åŒ–

#
X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma

âš ï¸ æ³¨æ„ï¼šæµ‹è¯•é›†å¿…é¡»ä½¿ç”¨è®­ç»ƒé›†çš„å‡å€¼å’Œæ ‡å‡†å·®è¿›è¡Œæ ‡å‡†åŒ–ï¼Œé¿å…æ•°æ®æ³„éœ²

ğŸ“ˆ çº¿æ€§å›å½’
ç†è®ºåŸºç¡€

çº¿æ€§å›å½’å‡è®¾ç›®æ ‡å˜é‡ä¸ç‰¹å¾å˜é‡ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼š
y = w0 + w1 x1 + w2 x2 + ... + wn xn + Ïµ
â€‹

â€‹

â€‹

â€‹

â€‹

â€‹

â€‹

å…¶ä¸­ï¼š
y æ˜¯ç›®æ ‡å˜é‡
æœºå™¨å­¦ä¹ 03

day03-

2

æ˜¯æƒé‡å‚æ•°
ï»¿æ˜¯ç‰¹å¾å˜é‡
Îµ æ˜¯è¯¯å·®é¡¹
wi
xi

ï»¿
â€‹

â€‹

æŸå¤±å‡½æ•°

çº¿æ€§å›å½’ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼š
n

1
MSE =

â€‹

n

âˆ‘(yi âˆ’ y
^i )
â€‹

â€‹

â€‹

2

â€‹

i=1

å®Œæ•´ä»£ç ç¤ºä¾‹
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
# 1.

æ•°æ®åŠ è½½

data = pd.read_csv("boston_house_prices.csv", skiprows=1)
# 2.

ç‰¹å¾å’Œæ ‡ç­¾åˆ†ç¦»

X = data.loc[:, :"LSTAT"].to_numpy() #
y = data.loc[:, "MEDV"].to_numpy() #
# 3.

ç‰¹å¾
æ ‡ç­¾

æ•°æ®åˆ‡åˆ†

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.2, random_state=0
)
# 4.

æ•°æ®æ ‡å‡†åŒ–

mu = X_train.mean(axis=0)
sigma = X_train.std(axis=0) + 1e-9
X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma
# 5.

æ¨¡å‹è®­ç»ƒ

lr = LinearRegression()
lr.fit(X=X_train, y=y_train)

é¢„æµ‹ä¸è¯„ä¼°

# 6.
y_pred = lr.predict(X=X_test)
mse = ((y_pred - y_test) ** 2).mean()
print(f"
: {mse}")

å‡æ–¹è¯¯å·®

ğŸ§  é€»è¾‘å›å½’ä¸åˆ†ç±»
Sigmoid

æ¿€æ´»å‡½æ•°

é€»è¾‘å›å½’çš„æ ¸å¿ƒæ˜¯ Sigmoid å‡½æ•°ï¼Œå®ƒå°†çº¿æ€§è¾“å‡ºæ˜ å°„åˆ° (0,1) åŒºé—´ï¼Œè¡¨ç¤ºæ¦‚ç‡ï¼š
1
Ïƒ(z) =

Sigmoid

å‡½æ•°ç‰¹æ€§

è¾“å‡ºèŒƒå›´ï¼š(0, 1)
Så‹æ›²çº¿ï¼šå¹³æ»‘çš„æ¦‚ç‡è½¬æ¢
å¯¼æ•°æ˜“è®¡ç®—ï¼š
â€²

Ïƒ (z) = Ïƒ(z)(1 âˆ’ Ïƒ(z))

æœºå™¨å­¦ä¹ 03

day03-

â€‹

1 + e

âˆ’z

ï»¿

3

ä»£ç å®ç°
import numpy as np
import matplotlib.pyplot as plt
def sigmoid(x):
"""Sigmoid

æ¿€æ´»å‡½æ•°"""

return 1 / (1 + np.exp(-x))

å¯è§†åŒ–

å‡½æ•°

#
Sigmoid
x = np.linspace(-10, 10, 100)
plt.plot(x, sigmoid(x))
plt.grid()
plt.title('Sigmoid Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.show()

é€»è¾‘å›å½’å®Œæ•´æµç¨‹
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

åŠ è½½æ•°æ®

# 1.
X, y = load_breast_cancer(return_X_y=True)
# 2.

æ•°æ®åˆ‡åˆ†

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.2, random_state=0
)
# 3.

æ•°æ®æ ‡å‡†åŒ–

mu = X_train.mean(axis=0)
sigma = X_train.std(axis=0) + 1e-9
X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma
# 4.

æ¨¡å‹è®­ç»ƒ

lr = LogisticRegression()
lr.fit(X=X_train, y=y_train)

é¢„æµ‹ä¸è¯„ä¼°

# 5.
y_pred = lr.predict(X=X_test)
accuracy = (y_pred == y_test).mean()
print(f"
: {accuracy:.4f}")

å‡†ç¡®ç‡
# æŸ¥çœ‹æ¨¡å‹å‚æ•°
print(f"æƒé‡æ•°é‡: {lr.coef_.shape}")
print(f"åç½®é¡¹: {lr.intercept_}")

ğŸ“Š æ¨¡å‹è¯„ä¼°

å›å½’é—®é¢˜è¯„ä¼°æŒ‡æ ‡
å‡æ–¹è¯¯å·® (MSE)
æœºå™¨å­¦ä¹ 03

day03-

4

n

1
MSE =

â€‹

n

âˆ‘(yi âˆ’ y
^i )
â€‹

â€‹

â€‹

2

â€‹

i=1

mse = ((y_pred - y_test) ** 2).mean()

åˆ†ç±»é—®é¢˜è¯„ä¼°æŒ‡æ ‡
å‡†ç¡®ç‡ (Accuracy)

Accuracy =

æ­£ç¡®é¢„æµ‹æ•°é‡
æ€»é¢„æµ‹æ•°é‡

â€‹

accuracy = (y_pred == y_test).mean()

ğŸ’¾ æ¨¡å‹ä¿å­˜ä¸åŠ è½½
åºåˆ—åŒ–æ¦‚å¿µ

åºåˆ—åŒ–ï¼šå°†å†…å­˜ä¸­çš„å¯¹è±¡è½¬åŒ–ä¸ºå­—èŠ‚æµï¼Œä¿å­˜åˆ°ç¡¬ç›˜
ååºåˆ—åŒ–ï¼šå°†ç¡¬ç›˜ä¸Šçš„æ–‡ä»¶è¯»å…¥ï¼Œè½¬åŒ–ä¸ºå†…å­˜ä¸­çš„å¯¹è±¡

Python

å·¥å…·

åºåˆ—åŒ–å·¥å…·å¯¹æ¯”

pickle
joblib

ç‰¹ç‚¹
é€‚ç”¨åœºæ™¯
åº•å±‚ï¼Œæ“ä½œç›¸å¯¹å¤æ‚ é€šç”¨Pythonå¯¹è±¡åºåˆ—åŒ–
ä¸Šå±‚ï¼Œæ“ä½œç®€ä¾¿
ä¸“ä¸ºå¤§å‹NumPyæ•°ç»„å’Œç§‘å­¦è®¡ç®—ä¼˜åŒ–

ä»£ç ç¤ºä¾‹

ä½¿ç”¨ pickle ä¿å­˜å•ä¸ªæ¨¡å‹ï¼š
import pickle

ä¿å­˜æ¨¡å‹

#
with open("lr.pickle", "wb") as f:
pickle.dump(obj=lr, file=f)
#

åŠ è½½æ¨¡å‹

with open("lr.pickle", "rb") as f:
lr_loaded = pickle.load(file=f)

ä½¿ç”¨ joblib ä¿å­˜å¤šä¸ªæ¨¡å‹ï¼š
import joblib

ä¿å­˜å¤šä¸ªæ¨¡å‹

#
joblib.dump(value=knn, filename="knn.joblib")
joblib.dump(value=[lr, knn], filename="models.joblib")

åŠ è½½æ¨¡å‹

#
knn_loaded = joblib.load(filename="knn.joblib")
lr_loaded, knn_loaded = joblib.load(filename="models.joblib")

ğŸ“ ç»Ÿè®¡å­¦æ¦‚å¿µï¼šæ–¹å·®
æ–¹å·®çš„ä¸¤ç§ä¼°è®¡æ–¹æ³•
æœºå™¨å­¦ä¹ 03

day03-

5

ç»Ÿè®¡å­¦ä¸­ï¼Œæ–¹å·®æœ‰ä¸¤ç§ä¸åŒçš„ä¼°è®¡æ–¹æ³•ï¼š

æ€»ä½“æ–¹å·®ï¼ˆæœ‰åä¼°è®¡ï¼‰

1.

Ïƒ

2

=

â€‹

N

2.

N

1

âˆ‘(xi âˆ’ Î¼)
â€‹

2

â€‹

i=1

æ ·æœ¬æ–¹å·®ï¼ˆæ— åä¼°è®¡ï¼‰
s

2

=

â€‹

N âˆ’ 1

åº“å‡½æ•°å·®å¼‚
åº“

N

1

âˆ‘(xi âˆ’ x
Ë‰)
â€‹

2

â€‹

i=1

é»˜è®¤æ–¹æ³•
å‚æ•°æ§åˆ¶
æœ‰åä¼°è®¡ (é™¤ä»¥N) ddof=1 ä½¿ç”¨æ— åä¼°è®¡
æ— åä¼°è®¡ (é™¤ä»¥N-1) correction=1 æ§åˆ¶

NumPy
PyTorch

ä»£ç ç¤ºä¾‹
import numpy as np
import torch
ls = [1, 2, 3, 4, 5, 6]

é»˜è®¤æœ‰åä¼°è®¡

# NumPy arr = np.array(ls)

æœ‰å

biased_std = arr.std()
#
unbiased_std = arr.std(ddof=1) #
# PyTorch -

æ— å

é»˜è®¤æ— åä¼°è®¡

t = torch.tensor(ls, dtype=torch.float32)
unbiased_std = t.std(correction=1) #

æ— å

âš ï¸ æ³¨æ„äº‹é¡¹
1.

é˜²æ­¢é™¤é›¶é”™è¯¯ï¼šæ–¹å·®è®¡ç®—æ—¶åŠ å…¥å°å¸¸æ•°
sigma = X_train.std(axis=0) + 1e-9

2.

é€‰æ‹©åˆé€‚çš„ä¼°è®¡æ–¹æ³•ï¼š
æ ·æœ¬é‡è¾ƒå°æ—¶ï¼Œä½¿ç”¨æ— åä¼°è®¡æ›´å‡†ç¡®
æ·±åº¦å­¦ä¹ ä¸­ï¼Œé€šå¸¸ä½¿ç”¨æœ‰åä¼°è®¡ï¼ˆè®¡ç®—æ•ˆç‡æ›´é«˜ï¼‰

ğŸš€ KNNç®—æ³•è¡¥å……
è¿‘é‚»ç®—æ³•ç‰¹ç‚¹

K

éå‚æ•°æ–¹æ³•ï¼šä¸éœ€è¦å‡è®¾æ•°æ®åˆ†å¸ƒ
æ‡’æƒ°å­¦ä¹ ï¼šè®­ç»ƒé˜¶æ®µåªå­˜å‚¨æ•°æ®ï¼Œé¢„æµ‹æ—¶æ‰è®¡ç®—
é€‚ç”¨äºï¼šå›å½’å’Œåˆ†ç±»é—®é¢˜

ä»£ç ç¤ºä¾‹

from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier

æœºå™¨å­¦ä¹ 03

day03-

6

å›å½’ä»»åŠ¡

#
knn_reg = KNeighborsRegressor(n_neighbors=5)
knn_reg.fit(X=X_train, y=y_train)

åˆ†ç±»ä»»åŠ¡

#
knn_clf = KNeighborsClassifier(n_neighbors=5)
knn_clf.fit(X=X_train, y=y_train)

ğŸ¯ å…³é”®è¦ç‚¹æ€»ç»“
âœ… æœ€ä½³å®è·µ

æ•°æ®é¢„å¤„ç†å¿…ä¸å¯å°‘ï¼šæ ‡å‡†åŒ–èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½
2. åˆç†åˆ’åˆ†æ•°æ®é›†ï¼šè®­ç»ƒé›†ç”¨äºå­¦ä¹ ï¼Œæµ‹è¯•é›†ç”¨äºè¯„ä¼°
3. é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡ï¼šMSEç”¨äºå›å½’ï¼Œå‡†ç¡®ç‡ç”¨äºåˆ†ç±»
4. æ¨¡å‹ä¿å­˜å¾ˆé‡è¦ï¼šä½¿ç”¨joblibä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
1.

ğŸ” æ·±å…¥ç†è§£

çº¿æ€§å›å½’ï¼šé€‚ç”¨äºçº¿æ€§å…³ç³»æ˜æ˜¾çš„å›å½’é—®é¢˜
2. é€»è¾‘å›å½’ï¼šé€šè¿‡Sigmoidå‡½æ•°å®ç°åˆ†ç±»ï¼Œæœ¬è´¨ä¸Šæ˜¯çº¿æ€§åˆ†ç±»å™¨
3. æ–¹å·®ä¼°è®¡ï¼šç†è§£æœ‰åä¸æ— åçš„åŒºåˆ«ï¼Œæ ¹æ®åœºæ™¯é€‰æ‹©
1.

ğŸ“ˆ è¿›é˜¶æ–¹å‘

æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆRidgeã€Lassoï¼‰
æ¨¡å‹é€‰æ‹©ä¸äº¤å‰éªŒè¯
ç‰¹å¾å·¥ç¨‹æŠ€å·§
é›†æˆå­¦ä¹ æ–¹æ³•

æœºå™¨å­¦ä¹ 03

day03-

7


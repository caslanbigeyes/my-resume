---
title: "机器学习03 - AIE54 Day03学习笔记"
date: "2024-06-17"
summary: "机器学习第三天学习内容，包括数据预处理、线性回归、逻辑回归、模型评估和模型保存"
tags: ["机器学习", "线性回归", "逻辑回归", "数据预处理", "模型评估", "AIE54"]
readingTime: 18
hasImages: true
slug: "2024-06-17"
---

# 机器学习03 - AIE54 Day03学习笔记

> **学习日期**：2024-06-17  
> **主讲老师**：李晓华  
> **课时**：2  
> **文档来源**：day03-机器学习03.pdf

## 课程关键词
机器学习 | 数据预处理 | 线性回归 | 逻辑回归 | 模型评估 | KNN算法

---

## 🎯 第一部分：机器学习基础概念

### 1.1 有监督学习流程

**标准机器学习流程**包括以下关键步骤：

1. **数据收集与加载**
2. **特征工程与数据预处理**
3. **模型选择与训练**
4. **模型评估与优化**
5. **模型部署与保存**

### 1.2 回归 vs 分类

**核心区别**：

- **回归问题**：预测连续数值（如房价预测、温度预测）
- **分类问题**：预测离散类别（如疾病诊断、图像识别）

**应用场景**：
- 回归：股价预测、销量预测、年龄估计
- 分类：垃圾邮件检测、情感分析、医疗诊断

---

## 🔧 第二部分：数据预处理与标准化

### 2.1 数据标准化的重要性

**核心目的**：消除不同特征之间量纲差异的影响，确保所有特征在相同尺度上参与模型训练。

**为什么需要标准化？**
- 不同特征的数值范围可能相差巨大
- 大数值特征会主导模型学习过程
- 影响基于距离的算法（如KNN、SVM）

### 2.2 标准化公式

**Z-score 标准化公式**：

X_normalized = (X - μ) / (σ + ε)

其中：
- μ 是均值
- σ 是标准差  
- ε = 1e-9 是防止除零的小常数

### 2.3 代码实现

```python
# 计算训练集的均值和标准差
mu = X_train.mean(axis=0)
sigma = X_train.std(axis=0) + 1e-9

# 对训练集和测试集进行标准化
X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma
```

**⚠️ 重要注意事项**：
- 测试集必须使用训练集的均值和标准差进行标准化
- 避免数据泄露问题
- 保证模型在真实环境中的泛化能力

---

## 📈 第三部分：线性回归

### 3.1 理论基础

**线性回归假设**：目标变量与特征变量之间存在线性关系

**数学表达式**：
y = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ + ε

其中：
- y 是目标变量
- wᵢ 是权重参数
- xᵢ 是特征变量
- ε 是误差项

### 3.2 损失函数

**均方误差（MSE）**作为损失函数：

MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²

**特点**：
- 对异常值敏感
- 可导，便于优化
- 几何意义明确

### 3.3 完整代码示例

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# 1. 数据加载
data = pd.read_csv("boston_house_prices.csv", skiprows=1)

# 2. 特征和标签分离
X = data.loc[:, :"LSTAT"].to_numpy()  # 特征
y = data.loc[:, "MEDV"].to_numpy()    # 标签

# 3. 数据切分
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0
)

# 4. 数据标准化
mu = X_train.mean(axis=0)
sigma = X_train.std(axis=0) + 1e-9
X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma

# 5. 模型训练
lr = LinearRegression()
lr.fit(X=X_train, y=y_train)

# 6. 预测与评估
y_pred = lr.predict(X=X_test)
mse = ((y_pred - y_test) ** 2).mean()
print(f"均方误差: {mse}")
```

---

## 🧠 第四部分：逻辑回归与分类

### 4.1 Sigmoid 激活函数

**核心作用**：将线性输出映射到 (0,1) 区间，表示概率

**数学公式**：
σ(z) = 1 / (1 + e^(-z))

### 4.2 Sigmoid 函数特性

**重要特性**：
- **输出范围**：(0, 1)
- **S型曲线**：平滑的概率转换
- **导数易计算**：σ'(z) = σ(z)(1 - σ(z))
- **单调递增**：适合概率建模

### 4.3 代码实现

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    """Sigmoid 激活函数"""
    return 1 / (1 + np.exp(-x))

# 可视化 Sigmoid 函数
x = np.linspace(-10, 10, 100)
plt.plot(x, sigmoid(x))
plt.grid()
plt.title('Sigmoid Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.show()
```

### 4.4 逻辑回归完整流程

```python
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# 1. 加载数据
X, y = load_breast_cancer(return_X_y=True)

# 2. 数据切分
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0
)

# 3. 数据标准化
mu = X_train.mean(axis=0)
sigma = X_train.std(axis=0) + 1e-9
X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma

# 4. 模型训练
lr = LogisticRegression()
lr.fit(X=X_train, y=y_train)

# 5. 预测与评估
y_pred = lr.predict(X=X_test)
accuracy = (y_pred == y_test).mean()
print(f"准确率: {accuracy:.4f}")

# 查看模型参数
print(f"权重数量: {lr.coef_.shape}")
print(f"偏置项: {lr.intercept_}")
```

---

## 📊 第五部分：模型评估

### 5.1 回归问题评估指标

**均方误差 (MSE)**：

MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²

```python
mse = ((y_pred - y_test) ** 2).mean()
```

**其他常用指标**：
- **MAE**：平均绝对误差
- **RMSE**：均方根误差
- **R²**：决定系数

### 5.2 分类问题评估指标

**准确率 (Accuracy)**：

Accuracy = 正确预测数量 / 总预测数量

```python
accuracy = (y_pred == y_test).mean()
```

**其他重要指标**：
- **精确率 (Precision)**
- **召回率 (Recall)**
- **F1-Score**
- **AUC-ROC**

---

## 💾 第六部分：模型保存与加载

### 6.1 序列化概念

**核心概念**：
- **序列化**：将内存中的对象转化为字节流，保存到硬盘
- **反序列化**：将硬盘上的文件读入，转化为内存中的对象

### 6.2 Python 序列化工具对比

| 工具 | 特点 | 适用场景 |
|------|------|----------|
| **pickle** | 底层，操作相对复杂 | 通用Python对象序列化 |
| **joblib** | 上层，操作简便 | 专为大型NumPy数组和科学计算优化 |

### 6.3 代码示例

**使用 pickle 保存单个模型**：

```python
import pickle

# 保存模型
with open("lr.pickle", "wb") as f:
    pickle.dump(obj=lr, file=f)

# 加载模型
with open("lr.pickle", "rb") as f:
    lr_loaded = pickle.load(file=f)
```

**使用 joblib 保存多个模型**：

```python
import joblib

# 保存多个模型
joblib.dump(value=knn, filename="knn.joblib")
joblib.dump(value=[lr, knn], filename="models.joblib")

# 加载模型
knn_loaded = joblib.load(filename="knn.joblib")
lr_loaded, knn_loaded = joblib.load(filename="models.joblib")
```

---

## 📐 第七部分：统计学概念：方差

### 7.1 方差的两种估计方法

**1. 总体方差（有偏估计）**：

σ² = (1/N) × Σᵢ₌₁ᴺ (xᵢ - μ)²

**2. 样本方差（无偏估计）**：

s² = (1/(N-1)) × Σᵢ₌₁ᴺ (xᵢ - x̄)²

### 7.2 库函数差异

| 库 | 默认方法 | 参数控制 |
|-----|----------|----------|
| **NumPy** | 有偏估计 (除以N) | ddof=1 使用无偏估计 |
| **PyTorch** | 无偏估计 (除以N-1) | correction=1 控制 |

### 7.3 代码示例

```python
import numpy as np
import torch

ls = [1, 2, 3, 4, 5, 6]

# NumPy - 默认有偏估计
arr = np.array(ls)
biased_std = arr.std()        # 有偏
unbiased_std = arr.std(ddof=1)  # 无偏

# PyTorch - 默认无偏估计
t = torch.tensor(ls, dtype=torch.float32)
unbiased_std = t.std(correction=1)  # 无偏
```

### 7.4 注意事项

1. **防止除零错误**：
```python
sigma = X_train.std(axis=0) + 1e-9
```

2. **选择合适的估计方法**：
   - 样本量较小时，使用无偏估计更准确
   - 深度学习中，通常使用有偏估计（计算效率更高）

---

## 🚀 第八部分：KNN算法补充

### 8.1 K近邻算法特点

**核心特性**：
- **非参数方法**：不需要假设数据分布
- **懒惰学习**：训练阶段只存储数据，预测时才计算
- **适用性广**：回归和分类问题都适用

### 8.2 代码示例

```python
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier

# 回归任务
knn_reg = KNeighborsRegressor(n_neighbors=5)
knn_reg.fit(X=X_train, y=y_train)

# 分类任务
knn_clf = KNeighborsClassifier(n_neighbors=5)
knn_clf.fit(X=X_train, y=y_train)
```

---

## 🎯 关键要点总结

### ✅ 最佳实践

1. **数据预处理必不可少**：标准化能显著提升模型性能
2. **合理划分数据集**：训练集用于学习，测试集用于评估
3. **选择合适的评估指标**：MSE用于回归，准确率用于分类
4. **模型保存很重要**：使用joblib保存训练好的模型

### 🔍 深入理解

1. **线性回归**：适用于线性关系明显的回归问题
2. **逻辑回归**：通过Sigmoid函数实现分类，本质上是线性分类器
3. **方差估计**：理解有偏与无偏的区别，根据场景选择

### 📈 进阶方向

- **正则化方法**（Ridge、Lasso）
- **模型选择与交叉验证**
- **特征工程技巧**
- **集成学习方法**

---

## 学习心得

这节课深入学习了机器学习的核心算法和实践技巧：

1. **数据预处理的重要性**：标准化不仅是技术要求，更是保证模型公平性的关键步骤
2. **线性模型的威力**：虽然简单，但线性回归和逻辑回归在很多实际问题中都表现优异
3. **模型评估的科学性**：选择合适的评估指标对模型性能判断至关重要
4. **工程实践的价值**：模型保存、加载等工程技巧是实际部署的必备技能

## 下一步学习计划
- 深入学习正则化技术
- 探索更复杂的非线性模型
- 实践特征工程技巧
- 学习模型调优方法

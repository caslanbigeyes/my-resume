{
  "title": "LLM å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ è·¯å¾„å®Œå…¨æŒ‡å—ï¼šä»å…¥é—¨åˆ°å®æˆ˜",
  "excerpt": "å…¨é¢çš„å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ è·¯å¾„ï¼Œæ¶µç›–åŸºç¡€ç†è®ºã€æŠ€æœ¯æ ˆã€å®æˆ˜é¡¹ç›®å’ŒèŒä¸šå‘å±•ï¼Œå¸®åŠ©å¼€å‘è€…ç³»ç»Ÿæ€§æŒæ¡ LLM æŠ€æœ¯ã€‚",
  "publishedAt": "2025-01-22T00:00:00.000Z",
  "author": "li-lingfeng",
  "category": "ai",
  "tags": [
    "llm",
    "ai",
    "machine-learning",
    "deep-learning",
    "nlp"
  ],
  "featured": true,
  "published": true,
  "image": "/images/articles/llm-roadmap.jpg",
  "seoTitle": "LLM å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ è·¯å¾„ - ä»é›¶åŸºç¡€åˆ° AI å·¥ç¨‹å¸ˆ",
  "seoDescription": "å®Œæ•´çš„ LLM å­¦ä¹ æŒ‡å—ï¼ŒåŒ…æ‹¬æ•°å­¦åŸºç¡€ã€æ·±åº¦å­¦ä¹ ã€Transformerã€å¾®è°ƒæŠ€æœ¯å’Œå®æˆ˜é¡¹ç›®",
  "seoKeywords": [
    "LLM",
    "å¤§è¯­è¨€æ¨¡å‹",
    "AIå­¦ä¹ ",
    "æ·±åº¦å­¦ä¹ ",
    "Transformer",
    "ChatGPT"
  ],
  "body": {
    "raw": "\n# LLM å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ è·¯å¾„å®Œå…¨æŒ‡å—\n\néšç€ ChatGPTã€GPT-4ã€Claude ç­‰å¤§è¯­è¨€æ¨¡å‹çš„çˆ†ç«ï¼ŒLLM æŠ€æœ¯å·²æˆä¸º AI é¢†åŸŸæœ€çƒ­é—¨çš„æ–¹å‘ã€‚æœ¬æ–‡å°†ä¸ºæ‚¨æä¾›ä¸€ä¸ªç³»ç»Ÿæ€§çš„ LLM å­¦ä¹ è·¯å¾„ï¼Œä»åŸºç¡€ç†è®ºåˆ°å®æˆ˜åº”ç”¨ï¼Œå¸®åŠ©æ‚¨æˆä¸º LLM é¢†åŸŸçš„ä¸“å®¶ã€‚\n\n## ğŸ¯ å­¦ä¹ ç›®æ ‡è®¾å®š\n\n### åˆçº§ç›®æ ‡ï¼ˆ0-3ä¸ªæœˆï¼‰\n- ç†è§£ LLM çš„åŸºæœ¬æ¦‚å¿µå’Œå·¥ä½œåŸç†\n- æŒæ¡åŸºç¡€çš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çŸ¥è¯†\n- èƒ½å¤Ÿä½¿ç”¨ç°æœ‰çš„ LLM API è¿›è¡Œç®€å•åº”ç”¨å¼€å‘\n\n### ä¸­çº§ç›®æ ‡ï¼ˆ3-8ä¸ªæœˆï¼‰\n- æ·±å…¥ç†è§£ Transformer æ¶æ„\n- æŒæ¡æ¨¡å‹å¾®è°ƒï¼ˆFine-tuningï¼‰æŠ€æœ¯\n- èƒ½å¤Ÿéƒ¨ç½²å’Œä¼˜åŒ– LLM æ¨¡å‹\n\n### é«˜çº§ç›®æ ‡ï¼ˆ8-18ä¸ªæœˆï¼‰\n- ç†è§£ LLM çš„è®­ç»ƒè¿‡ç¨‹å’Œä¼˜åŒ–æŠ€æœ¯\n- æŒæ¡å¤šæ¨¡æ€å¤§æ¨¡å‹æŠ€æœ¯\n- èƒ½å¤Ÿä»é›¶å¼€å§‹è®­ç»ƒå°è§„æ¨¡è¯­è¨€æ¨¡å‹\n\n---\n\n## ğŸ“š ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€çŸ¥è¯†å»ºè®¾ï¼ˆ0-3ä¸ªæœˆï¼‰\n\n### 1.1 æ•°å­¦åŸºç¡€\n\n#### å¿…å¤‡æ•°å­¦çŸ¥è¯†\n```\nçº¿æ€§ä»£æ•° (é‡è¦åº¦: â­â­â­â­â­)\nâ”œâ”€â”€ å‘é‡å’ŒçŸ©é˜µè¿ç®—\nâ”œâ”€â”€ ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡\nâ”œâ”€â”€ çŸ©é˜µåˆ†è§£ï¼ˆSVDã€PCAï¼‰\nâ””â”€â”€ å‘é‡ç©ºé—´å’Œçº¿æ€§å˜æ¢\n\næ¦‚ç‡è®ºä¸ç»Ÿè®¡ (é‡è¦åº¦: â­â­â­â­â­)\nâ”œâ”€â”€ æ¦‚ç‡åˆ†å¸ƒ\nâ”œâ”€â”€ è´å¶æ–¯å®šç†\nâ”œâ”€â”€ æœ€å¤§ä¼¼ç„¶ä¼°è®¡\nâ””â”€â”€ ä¿¡æ¯è®ºåŸºç¡€\n\nå¾®ç§¯åˆ† (é‡è¦åº¦: â­â­â­â­)\nâ”œâ”€â”€ åå¯¼æ•°å’Œæ¢¯åº¦\nâ”œâ”€â”€ é“¾å¼æ³•åˆ™\nâ”œâ”€â”€ ä¼˜åŒ–ç†è®º\nâ””â”€â”€ æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•\n```\n\n#### æ¨èå­¦ä¹ èµ„æº\n- **ä¹¦ç±**ï¼šã€Šçº¿æ€§ä»£æ•°åŠå…¶åº”ç”¨ã€‹- David C. Lay\n- **åœ¨çº¿è¯¾ç¨‹**ï¼šKhan Academy æ•°å­¦è¯¾ç¨‹\n- **å®è·µå·¥å…·**ï¼šNumPyã€SciPy è¿›è¡Œæ•°å­¦è®¡ç®—ç»ƒä¹ \n\n### 1.2 ç¼–ç¨‹åŸºç¡€\n\n#### Python ç”Ÿæ€ç³»ç»Ÿ\n```python\n# æ ¸å¿ƒåº“æŒæ¡\nimport numpy as np          # æ•°å€¼è®¡ç®—\nimport pandas as pd         # æ•°æ®å¤„ç†\nimport matplotlib.pyplot as plt  # æ•°æ®å¯è§†åŒ–\nimport torch               # æ·±åº¦å­¦ä¹ æ¡†æ¶\nimport transformers        # Hugging Face åº“\n```\n\n#### å¿…å¤‡æŠ€èƒ½æ¸…å•\n- **Python é«˜çº§ç‰¹æ€§**ï¼šè£…é¥°å™¨ã€ç”Ÿæˆå™¨ã€ä¸Šä¸‹æ–‡ç®¡ç†å™¨\n- **æ•°æ®å¤„ç†**ï¼šPandasã€NumPy æ•°æ®æ“ä½œ\n- **å¯è§†åŒ–**ï¼šMatplotlibã€Seabornã€Plotly\n- **ç‰ˆæœ¬æ§åˆ¶**ï¼šGit å’Œ GitHub ä½¿ç”¨\n\n### 1.3 æœºå™¨å­¦ä¹ åŸºç¡€\n\n#### æ ¸å¿ƒæ¦‚å¿µç†è§£\n```\nç›‘ç£å­¦ä¹  vs æ— ç›‘ç£å­¦ä¹ \nâ”œâ”€â”€ åˆ†ç±»é—®é¢˜ï¼ˆClassificationï¼‰\nâ”œâ”€â”€ å›å½’é—®é¢˜ï¼ˆRegressionï¼‰\nâ”œâ”€â”€ èšç±»ï¼ˆClusteringï¼‰\nâ””â”€â”€ é™ç»´ï¼ˆDimensionality Reductionï¼‰\n\næ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–\nâ”œâ”€â”€ äº¤å‰éªŒè¯ï¼ˆCross Validationï¼‰\nâ”œâ”€â”€ è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ\nâ”œâ”€â”€ æ­£åˆ™åŒ–æŠ€æœ¯\nâ””â”€â”€ è¶…å‚æ•°è°ƒä¼˜\n```\n\n#### å®è·µé¡¹ç›®\n1. **æ–‡æœ¬åˆ†ç±»é¡¹ç›®**ï¼šä½¿ç”¨ä¼ ç»Ÿ ML æ–¹æ³•è¿›è¡Œæƒ…æ„Ÿåˆ†æ\n2. **æ¨èç³»ç»Ÿ**ï¼šåŸºäºååŒè¿‡æ»¤çš„ç”µå½±æ¨è\n3. **æ•°æ®æŒ–æ˜**ï¼šæ–°é—»æ–‡æœ¬èšç±»åˆ†æ\n\n---\n\n## ğŸ§  ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦å­¦ä¹ ä¸ NLPï¼ˆ3-6ä¸ªæœˆï¼‰\n\n### 2.1 æ·±åº¦å­¦ä¹ åŸºç¡€\n\n#### ç¥ç»ç½‘ç»œæ¶æ„æ¼”è¿›\n```\nç¥ç»ç½‘ç»œå‘å±•å†ç¨‹\nâ”œâ”€â”€ æ„ŸçŸ¥æœºï¼ˆPerceptronï¼‰\nâ”œâ”€â”€ å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰\nâ”œâ”€â”€ å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰\nâ”œâ”€â”€ å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNN/LSTM/GRUï¼‰\nâ””â”€â”€ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰\n```\n\n#### PyTorch å®æˆ˜\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# æ¨¡å‹è®­ç»ƒç¤ºä¾‹\nmodel = SimpleNN(784, 128, 10)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n```\n\n### 2.2 è‡ªç„¶è¯­è¨€å¤„ç†åŸºç¡€\n\n#### NLP æ ¸å¿ƒä»»åŠ¡\n```\næ–‡æœ¬é¢„å¤„ç†\nâ”œâ”€â”€ åˆ†è¯ï¼ˆTokenizationï¼‰\nâ”œâ”€â”€ è¯æ€§æ ‡æ³¨ï¼ˆPOS Taggingï¼‰\nâ”œâ”€â”€ å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰\nâ””â”€â”€ å¥æ³•åˆ†æï¼ˆParsingï¼‰\n\næ–‡æœ¬è¡¨ç¤ºæ–¹æ³•\nâ”œâ”€â”€ è¯è¢‹æ¨¡å‹ï¼ˆBag of Wordsï¼‰\nâ”œâ”€â”€ TF-IDF\nâ”œâ”€â”€ Word2Vec\nâ”œâ”€â”€ GloVe\nâ””â”€â”€ FastText\n```\n\n#### å®è·µé¡¹ç›®\n1. **è¯å‘é‡è®­ç»ƒ**ï¼šä½¿ç”¨ Word2Vec è®­ç»ƒä¸­æ–‡è¯å‘é‡\n2. **æ–‡æœ¬ç›¸ä¼¼åº¦**ï¼šåŸºäºè¯å‘é‡çš„æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—\n3. **åºåˆ—æ ‡æ³¨**ï¼šä½¿ç”¨ LSTM è¿›è¡Œå‘½åå®ä½“è¯†åˆ«\n\n### 2.3 æ³¨æ„åŠ›æœºåˆ¶æ·±å…¥\n\n#### Attention æœºåˆ¶ç†è§£\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, hidden_size):\n        super(AttentionLayer, self).__init__()\n        self.hidden_size = hidden_size\n        self.W = nn.Linear(hidden_size, hidden_size)\n        \n    def forward(self, query, key, value):\n        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / (self.hidden_size ** 0.5)\n        \n        # åº”ç”¨ softmax\n        attention_weights = F.softmax(scores, dim=-1)\n        \n        # åŠ æƒæ±‚å’Œ\n        output = torch.matmul(attention_weights, value)\n        return output, attention_weights\n```\n\n---\n\n## ğŸš€ ç¬¬ä¸‰é˜¶æ®µï¼šTransformer ä¸ LLM æ ¸å¿ƒï¼ˆ6-10ä¸ªæœˆï¼‰\n\n### 3.1 Transformer æ¶æ„æ·±åº¦è§£æ\n\n#### æ ¸å¿ƒç»„ä»¶ç†è§£\n```\nTransformer æ¶æ„\nâ”œâ”€â”€ Multi-Head Attention\nâ”‚   â”œâ”€â”€ Self-Attention æœºåˆ¶\nâ”‚   â”œâ”€â”€ Queryã€Keyã€Value çŸ©é˜µ\nâ”‚   â””â”€â”€ å¤šå¤´æ³¨æ„åŠ›å¹¶è¡Œè®¡ç®—\nâ”œâ”€â”€ Position Encoding\nâ”‚   â”œâ”€â”€ ç»å¯¹ä½ç½®ç¼–ç \nâ”‚   â””â”€â”€ ç›¸å¯¹ä½ç½®ç¼–ç \nâ”œâ”€â”€ Feed Forward Network\nâ””â”€â”€ Layer Normalization\n```\n\n#### ä»é›¶å®ç° Transformer\n```python\nimport torch\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n            \n        attention_weights = torch.softmax(scores, dim=-1)\n        output = torch.matmul(attention_weights, V)\n        return output, attention_weights\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # çº¿æ€§å˜æ¢å¹¶é‡å¡‘ä¸ºå¤šå¤´\n        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # åº”ç”¨æ³¨æ„åŠ›\n        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # é‡å¡‘å¹¶åº”ç”¨è¾“å‡ºæŠ•å½±\n        attention_output = attention_output.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.d_model)\n        output = self.W_o(attention_output)\n        \n        return output, attention_weights\n```\n\n### 3.2 é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹\n\n#### æ¨¡å‹æ¶æ„å¯¹æ¯”\n```\nGPT ç³»åˆ—ï¼ˆç”Ÿæˆå¼ï¼‰\nâ”œâ”€â”€ GPT-1: 117M å‚æ•°\nâ”œâ”€â”€ GPT-2: 1.5B å‚æ•°\nâ”œâ”€â”€ GPT-3: 175B å‚æ•°\nâ””â”€â”€ GPT-4: å‚æ•°é‡æœªå…¬å¼€\n\nBERT ç³»åˆ—ï¼ˆç†è§£å¼ï¼‰\nâ”œâ”€â”€ BERT-Base: 110M å‚æ•°\nâ”œâ”€â”€ BERT-Large: 340M å‚æ•°\nâ””â”€â”€ RoBERTa: BERT çš„æ”¹è¿›ç‰ˆæœ¬\n\nT5 ç³»åˆ—ï¼ˆç¼–ç -è§£ç ï¼‰\nâ”œâ”€â”€ T5-Small: 60M å‚æ•°\nâ”œâ”€â”€ T5-Base: 220M å‚æ•°\nâ””â”€â”€ T5-Large: 770M å‚æ•°\n```\n\n#### ä½¿ç”¨ Hugging Face Transformers\n```python\nfrom transformers import (\n    AutoTokenizer, \n    AutoModel, \n    AutoModelForCausalLM,\n    pipeline\n)\n\n# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# æ–‡æœ¬ç”Ÿæˆ\ngenerator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nresult = generator(\"äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•\", max_length=100, num_return_sequences=1)\nprint(result[0]['generated_text'])\n\n# è‡ªå®šä¹‰æ¨ç†\ninput_text = \"æœºå™¨å­¦ä¹ æ˜¯\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    output = model.generate(\n        input_ids,\n        max_length=50,\n        num_return_sequences=1,\n        temperature=0.7,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(generated_text)\n```\n\n### 3.3 æ¨¡å‹å¾®è°ƒæŠ€æœ¯\n\n#### Fine-tuning ç­–ç•¥\n```\nå¾®è°ƒæ–¹æ³•åˆ†ç±»\nâ”œâ”€â”€ å…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-tuningï¼‰\nâ”œâ”€â”€ å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰\nâ”‚   â”œâ”€â”€ LoRAï¼ˆLow-Rank Adaptationï¼‰\nâ”‚   â”œâ”€â”€ Adapter Tuning\nâ”‚   â”œâ”€â”€ Prefix Tuning\nâ”‚   â””â”€â”€ P-Tuning v2\nâ””â”€â”€ æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰\n```\n\n#### LoRA å¾®è°ƒå®ç°\n```python\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n# é…ç½® LoRA\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=8,  # rank\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\n# åº”ç”¨ LoRA åˆ°æ¨¡å‹\nmodel = get_peft_model(model, lora_config)\n\n# è®­ç»ƒå¾ªç¯\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        optimizer.zero_grad()\n        \n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        \n        optimizer.step()\n        \n        if step % 100 == 0:\n            print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item()}\")\n```\n\n---\n\n## ğŸ› ï¸ ç¬¬å››é˜¶æ®µï¼šå®æˆ˜é¡¹ç›®ä¸åº”ç”¨ï¼ˆ10-15ä¸ªæœˆï¼‰\n\n### 4.1 LLM åº”ç”¨å¼€å‘\n\n#### é¡¹ç›®ä¸€ï¼šæ™ºèƒ½é—®ç­”ç³»ç»Ÿ\n```python\nimport openai\nfrom langchain import OpenAI, PromptTemplate, LLMChain\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import TextLoader\n\nclass IntelligentQA:\n    def __init__(self, api_key):\n        self.llm = OpenAI(openai_api_key=api_key)\n        self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n        self.vectorstore = None\n        \n    def build_knowledge_base(self, documents):\n        \"\"\"æ„å»ºçŸ¥è¯†åº“\"\"\"\n        self.vectorstore = FAISS.from_documents(documents, self.embeddings)\n        \n    def answer_question(self, question):\n        \"\"\"å›ç­”é—®é¢˜\"\"\"\n        if not self.vectorstore:\n            return \"çŸ¥è¯†åº“æœªåˆå§‹åŒ–\"\n            \n        # æ£€ç´¢ç›¸å…³æ–‡æ¡£\n        docs = self.vectorstore.similarity_search(question, k=3)\n        context = \"\\n\".join([doc.page_content for doc in docs])\n        \n        # æ„å»ºæç¤ºæ¨¡æ¿\n        template = \"\"\"\n        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n        \n        ä¸Šä¸‹æ–‡ï¼š{context}\n        \n        é—®é¢˜ï¼š{question}\n        \n        ç­”æ¡ˆï¼š\n        \"\"\"\n        \n        prompt = PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"]\n        )\n        \n        chain = LLMChain(llm=self.llm, prompt=prompt)\n        response = chain.run(context=context, question=question)\n        \n        return response\n\n# ä½¿ç”¨ç¤ºä¾‹\nqa_system = IntelligentQA(\"your-api-key\")\n# åŠ è½½æ–‡æ¡£å¹¶æ„å»ºçŸ¥è¯†åº“\n# qa_system.build_knowledge_base(documents)\n# answer = qa_system.answer_question(\"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\")\n```\n\n#### é¡¹ç›®äºŒï¼šä»£ç ç”ŸæˆåŠ©æ‰‹\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nclass CodeGenerator:\n    def __init__(self, model_name=\"microsoft/CodeGPT-small-py\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        \n    def generate_code(self, prompt, max_length=200):\n        \"\"\"ç”Ÿæˆä»£ç \"\"\"\n        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs,\n                max_length=max_length,\n                num_return_sequences=1,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n        \n        generated_code = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return generated_code[len(prompt):]\n    \n    def explain_code(self, code):\n        \"\"\"è§£é‡Šä»£ç \"\"\"\n        prompt = f\"è¯·è§£é‡Šä»¥ä¸‹ä»£ç çš„åŠŸèƒ½ï¼š\\n{code}\\nè§£é‡Šï¼š\"\n        return self.generate_code(prompt)\n\n# ä½¿ç”¨ç¤ºä¾‹\ncode_gen = CodeGenerator()\nprompt = \"# å®ç°å¿«é€Ÿæ’åºç®—æ³•\\ndef quicksort(arr):\"\ngenerated = code_gen.generate_code(prompt)\nprint(generated)\n```\n\n### 4.2 æ¨¡å‹éƒ¨ç½²ä¸ä¼˜åŒ–\n\n#### æ¨¡å‹é‡åŒ–\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndef quantize_model(model_path, output_path):\n    \"\"\"æ¨¡å‹é‡åŒ–\"\"\"\n    # åŠ è½½æ¨¡å‹\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    \n    # åŠ¨æ€é‡åŒ–\n    quantized_model = torch.quantization.quantize_dynamic(\n        model, \n        {torch.nn.Linear}, \n        dtype=torch.qint8\n    )\n    \n    # ä¿å­˜é‡åŒ–æ¨¡å‹\n    torch.save(quantized_model.state_dict(), output_path)\n    \n    return quantized_model\n\n# æ¨¡å‹æ¨ç†ä¼˜åŒ–\nclass OptimizedInference:\n    def __init__(self, model_path):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForCausalLM.from_pretrained(model_path)\n        \n        # å¯ç”¨æ¨ç†ä¼˜åŒ–\n        self.model.eval()\n        if torch.cuda.is_available():\n            self.model = self.model.cuda()\n            \n    @torch.no_grad()\n    def generate(self, prompt, **kwargs):\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n            \n        outputs = self.model.generate(**inputs, **kwargs)\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n```\n\n---\n\n## ğŸ“ ç¬¬äº”é˜¶æ®µï¼šé«˜çº§æŠ€æœ¯ä¸ç ”ç©¶ï¼ˆ15ä¸ªæœˆ+ï¼‰\n\n### 5.1 å¤šæ¨¡æ€å¤§æ¨¡å‹\n\n#### è§†è§‰-è¯­è¨€æ¨¡å‹\n```python\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\nclass MultimodalModel:\n    def __init__(self):\n        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    \n    def image_to_text(self, image_path):\n        \"\"\"å›¾åƒæè¿°ç”Ÿæˆ\"\"\"\n        image = Image.open(image_path)\n        inputs = self.processor(image, return_tensors=\"pt\")\n        \n        out = self.model.generate(**inputs, max_length=50)\n        caption = self.processor.decode(out[0], skip_special_tokens=True)\n        \n        return caption\n    \n    def visual_question_answering(self, image_path, question):\n        \"\"\"è§†è§‰é—®ç­”\"\"\"\n        image = Image.open(image_path)\n        inputs = self.processor(image, question, return_tensors=\"pt\")\n        \n        out = self.model.generate(**inputs, max_length=50)\n        answer = self.processor.decode(out[0], skip_special_tokens=True)\n        \n        return answer\n```\n\n### 5.2 å¼ºåŒ–å­¦ä¹ ä¸ RLHF\n\n#### äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ \n```python\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass RLHFTrainer:\n    def __init__(self, model_name):\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.reward_model = self.build_reward_model()\n        \n    def build_reward_model(self):\n        \"\"\"æ„å»ºå¥–åŠ±æ¨¡å‹\"\"\"\n        class RewardModel(nn.Module):\n            def __init__(self, base_model):\n                super().__init__()\n                self.base_model = base_model\n                self.reward_head = nn.Linear(base_model.config.hidden_size, 1)\n                \n            def forward(self, input_ids, attention_mask=None):\n                outputs = self.base_model(input_ids, attention_mask=attention_mask)\n                rewards = self.reward_head(outputs.last_hidden_state)\n                return rewards.squeeze(-1)\n        \n        return RewardModel(self.model)\n    \n    def ppo_step(self, prompts, responses, rewards):\n        \"\"\"PPO è®­ç»ƒæ­¥éª¤\"\"\"\n        # è®¡ç®—ç­–ç•¥æ¢¯åº¦\n        # å®ç° PPO ç®—æ³•\n        pass\n```\n\n---\n\n## ğŸ“ˆ å­¦ä¹ èµ„æºæ¨è\n\n### ğŸ“– å¿…è¯»ä¹¦ç±\n1. **ã€Šæ·±åº¦å­¦ä¹ ã€‹** - Ian Goodfellow\n2. **ã€Šè‡ªç„¶è¯­è¨€å¤„ç†ç»¼è®ºã€‹** - Daniel Jurafsky\n3. **ã€ŠAttention Is All You Needã€‹** - Transformer åŸè®ºæ–‡\n4. **ã€ŠLanguage Models are Few-Shot Learnersã€‹** - GPT-3 è®ºæ–‡\n\n### ğŸ¥ åœ¨çº¿è¯¾ç¨‹\n1. **CS224N: Natural Language Processing with Deep Learning** (Stanford)\n2. **CS231N: Convolutional Neural Networks** (Stanford)\n3. **Fast.ai Deep Learning Course**\n4. **Hugging Face Course**\n\n### ğŸ› ï¸ å®è·µå¹³å°\n1. **Hugging Face Hub** - æ¨¡å‹å’Œæ•°æ®é›†\n2. **Google Colab** - å…è´¹ GPU è®­ç»ƒ\n3. **Kaggle** - ç«èµ›å’Œæ•°æ®é›†\n4. **Papers With Code** - è®ºæ–‡å’Œä»£ç \n\n### ğŸŒ ç¤¾åŒºèµ„æº\n1. **GitHub** - å¼€æºé¡¹ç›®å’Œä»£ç \n2. **Reddit r/MachineLearning** - å­¦æœ¯è®¨è®º\n3. **Twitter** - æœ€æ–°ç ”ç©¶åŠ¨æ€\n4. **çŸ¥ä¹/CSDN** - ä¸­æ–‡æŠ€æœ¯ç¤¾åŒº\n\n---\n\n## ğŸš€ èŒä¸šå‘å±•è·¯å¾„\n\n### æŠ€æœ¯å²—ä½\n- **AI å·¥ç¨‹å¸ˆ**ï¼šæ¨¡å‹å¼€å‘å’Œéƒ¨ç½²\n- **ç®—æ³•å·¥ç¨‹å¸ˆ**ï¼šç®—æ³•ç ”ç©¶å’Œä¼˜åŒ–\n- **æ•°æ®ç§‘å­¦å®¶**ï¼šæ•°æ®åˆ†æå’Œå»ºæ¨¡\n- **ç ”ç©¶ç§‘å­¦å®¶**ï¼šå‰æ²¿æŠ€æœ¯ç ”ç©¶\n\n### èƒ½åŠ›è¦æ±‚\n- **æŠ€æœ¯æ·±åº¦**ï¼šæ·±å…¥ç†è§£ LLM åŸç†å’Œå®ç°\n- **å·¥ç¨‹èƒ½åŠ›**ï¼šå¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡å’Œä¼˜åŒ–\n- **ç ”ç©¶èƒ½åŠ›**ï¼šè·Ÿè¸ªå‰æ²¿æŠ€æœ¯å’Œåˆ›æ–°\n- **æ²Ÿé€šèƒ½åŠ›**ï¼šæŠ€æœ¯æ–¹æ¡ˆè¡¨è¾¾å’Œå›¢é˜Ÿåä½œ\n\n### è–ªèµ„æ°´å¹³ï¼ˆ2024å¹´ï¼‰\n- **åˆçº§**ï¼š20-40ä¸‡/å¹´\n- **ä¸­çº§**ï¼š40-80ä¸‡/å¹´\n- **é«˜çº§**ï¼š80-150ä¸‡/å¹´\n- **ä¸“å®¶**ï¼š150ä¸‡+/å¹´\n\n---\n\n## ğŸ¯ å­¦ä¹ å»ºè®®ä¸æ€»ç»“\n\n### å­¦ä¹ ç­–ç•¥\n1. **ç†è®ºä¸å®è·µå¹¶é‡**ï¼šä¸è¦åªçœ‹è®ºæ–‡ï¼Œè¦åŠ¨æ‰‹å®ç°\n2. **å¾ªåºæ¸è¿›**ï¼šä»ç®€å•æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥æ·±å…¥\n3. **é¡¹ç›®é©±åŠ¨**ï¼šé€šè¿‡å®é™…é¡¹ç›®å·©å›ºçŸ¥è¯†\n4. **æŒç»­å­¦ä¹ **ï¼šå…³æ³¨æœ€æ–°ç ”ç©¶å’ŒæŠ€æœ¯å‘å±•\n\n### å¸¸è§è¯¯åŒº\n- âŒ æ€¥äºæ±‚æˆï¼Œè·³è¿‡åŸºç¡€çŸ¥è¯†\n- âŒ åªå…³æ³¨æœ€æ–°æŠ€æœ¯ï¼Œå¿½è§†åŸºç¡€åŸç†\n- âŒ çº¸ä¸Šè°ˆå…µï¼Œç¼ºä¹å®é™…ç¼–ç¨‹ç»éªŒ\n- âŒ å­¤å†›å¥‹æˆ˜ï¼Œä¸å‚ä¸æŠ€æœ¯ç¤¾åŒº\n\n### æˆåŠŸè¦ç´ \n- âœ… æ‰å®çš„æ•°å­¦å’Œç¼–ç¨‹åŸºç¡€\n- âœ… æŒç»­çš„å­¦ä¹ å’Œå®è·µ\n- âœ… ç§¯æçš„æŠ€æœ¯ç¤¾åŒºå‚ä¸\n- âœ… æ¸…æ™°çš„èŒä¸šè§„åˆ’å’Œç›®æ ‡\n\nLLM æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œè¿™æ˜¯ä¸€ä¸ªå……æ»¡æœºé‡çš„é¢†åŸŸã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„å­¦ä¹ å’ŒæŒç»­çš„å®è·µï¼Œæ‚¨ä¸€å®šèƒ½å¤Ÿåœ¨è¿™ä¸ªæ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸä¸­å–å¾—æˆåŠŸï¼ğŸš€\n\nè®°ä½ï¼š**å­¦ä¹  LLM ä¸æ˜¯ç»ˆç‚¹ï¼Œè€Œæ˜¯å¼€å¯ AI æ—¶ä»£çš„èµ·ç‚¹**ã€‚ä¿æŒå¥½å¥‡å¿ƒï¼ŒæŒç»­å­¦ä¹ ï¼Œæ‹¥æŠ±å˜åŒ–ï¼Œæ‚¨å°†åœ¨è¿™ä¸ªé¢†åŸŸä¸­æ‰¾åˆ°å±äºè‡ªå·±çš„ä½ç½®ã€‚\n",
    "html": "<h1>LLM å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ è·¯å¾„å®Œå…¨æŒ‡å—</h1>\n<p>éšç€ ChatGPTã€GPT-4ã€Claude ç­‰å¤§è¯­è¨€æ¨¡å‹çš„çˆ†ç«ï¼ŒLLM æŠ€æœ¯å·²æˆä¸º AI é¢†åŸŸæœ€çƒ­é—¨çš„æ–¹å‘ã€‚æœ¬æ–‡å°†ä¸ºæ‚¨æä¾›ä¸€ä¸ªç³»ç»Ÿæ€§çš„ LLM å­¦ä¹ è·¯å¾„ï¼Œä»åŸºç¡€ç†è®ºåˆ°å®æˆ˜åº”ç”¨ï¼Œå¸®åŠ©æ‚¨æˆä¸º LLM é¢†åŸŸçš„ä¸“å®¶ã€‚</p>\n<h2>ğŸ¯ å­¦ä¹ ç›®æ ‡è®¾å®š</h2>\n<h3>åˆçº§ç›®æ ‡ï¼ˆ0-3ä¸ªæœˆï¼‰</h3>\n<ul>\n<li>ç†è§£ LLM çš„åŸºæœ¬æ¦‚å¿µå’Œå·¥ä½œåŸç†</li>\n<li>æŒæ¡åŸºç¡€çš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çŸ¥è¯†</li>\n<li>èƒ½å¤Ÿä½¿ç”¨ç°æœ‰çš„ LLM API è¿›è¡Œç®€å•åº”ç”¨å¼€å‘</li>\n</ul>\n<h3>ä¸­çº§ç›®æ ‡ï¼ˆ3-8ä¸ªæœˆï¼‰</h3>\n<ul>\n<li>æ·±å…¥ç†è§£ Transformer æ¶æ„</li>\n<li>æŒæ¡æ¨¡å‹å¾®è°ƒï¼ˆFine-tuningï¼‰æŠ€æœ¯</li>\n<li>èƒ½å¤Ÿéƒ¨ç½²å’Œä¼˜åŒ– LLM æ¨¡å‹</li>\n</ul>\n<h3>é«˜çº§ç›®æ ‡ï¼ˆ8-18ä¸ªæœˆï¼‰</h3>\n<ul>\n<li>ç†è§£ LLM çš„è®­ç»ƒè¿‡ç¨‹å’Œä¼˜åŒ–æŠ€æœ¯</li>\n<li>æŒæ¡å¤šæ¨¡æ€å¤§æ¨¡å‹æŠ€æœ¯</li>\n<li>èƒ½å¤Ÿä»é›¶å¼€å§‹è®­ç»ƒå°è§„æ¨¡è¯­è¨€æ¨¡å‹</li>\n</ul>\n<hr>\n<h2>ğŸ“š ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€çŸ¥è¯†å»ºè®¾ï¼ˆ0-3ä¸ªæœˆï¼‰</h2>\n<h3>1.1 æ•°å­¦åŸºç¡€</h3>\n<h4>å¿…å¤‡æ•°å­¦çŸ¥è¯†</h4>\n<pre><code>çº¿æ€§ä»£æ•° (é‡è¦åº¦: â­â­â­â­â­)\nâ”œâ”€â”€ å‘é‡å’ŒçŸ©é˜µè¿ç®—\nâ”œâ”€â”€ ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡\nâ”œâ”€â”€ çŸ©é˜µåˆ†è§£ï¼ˆSVDã€PCAï¼‰\nâ””â”€â”€ å‘é‡ç©ºé—´å’Œçº¿æ€§å˜æ¢\n\næ¦‚ç‡è®ºä¸ç»Ÿè®¡ (é‡è¦åº¦: â­â­â­â­â­)\nâ”œâ”€â”€ æ¦‚ç‡åˆ†å¸ƒ\nâ”œâ”€â”€ è´å¶æ–¯å®šç†\nâ”œâ”€â”€ æœ€å¤§ä¼¼ç„¶ä¼°è®¡\nâ””â”€â”€ ä¿¡æ¯è®ºåŸºç¡€\n\nå¾®ç§¯åˆ† (é‡è¦åº¦: â­â­â­â­)\nâ”œâ”€â”€ åå¯¼æ•°å’Œæ¢¯åº¦\nâ”œâ”€â”€ é“¾å¼æ³•åˆ™\nâ”œâ”€â”€ ä¼˜åŒ–ç†è®º\nâ””â”€â”€ æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•\n</code></pre>\n<h4>æ¨èå­¦ä¹ èµ„æº</h4>\n<ul>\n<li><strong>ä¹¦ç±</strong>ï¼šã€Šçº¿æ€§ä»£æ•°åŠå…¶åº”ç”¨ã€‹- David C. Lay</li>\n<li><strong>åœ¨çº¿è¯¾ç¨‹</strong>ï¼šKhan Academy æ•°å­¦è¯¾ç¨‹</li>\n<li><strong>å®è·µå·¥å…·</strong>ï¼šNumPyã€SciPy è¿›è¡Œæ•°å­¦è®¡ç®—ç»ƒä¹ </li>\n</ul>\n<h3>1.2 ç¼–ç¨‹åŸºç¡€</h3>\n<h4>Python ç”Ÿæ€ç³»ç»Ÿ</h4>\n<pre><code class=\"language-python\"># æ ¸å¿ƒåº“æŒæ¡\nimport numpy as np          # æ•°å€¼è®¡ç®—\nimport pandas as pd         # æ•°æ®å¤„ç†\nimport matplotlib.pyplot as plt  # æ•°æ®å¯è§†åŒ–\nimport torch               # æ·±åº¦å­¦ä¹ æ¡†æ¶\nimport transformers        # Hugging Face åº“\n</code></pre>\n<h4>å¿…å¤‡æŠ€èƒ½æ¸…å•</h4>\n<ul>\n<li><strong>Python é«˜çº§ç‰¹æ€§</strong>ï¼šè£…é¥°å™¨ã€ç”Ÿæˆå™¨ã€ä¸Šä¸‹æ–‡ç®¡ç†å™¨</li>\n<li><strong>æ•°æ®å¤„ç†</strong>ï¼šPandasã€NumPy æ•°æ®æ“ä½œ</li>\n<li><strong>å¯è§†åŒ–</strong>ï¼šMatplotlibã€Seabornã€Plotly</li>\n<li><strong>ç‰ˆæœ¬æ§åˆ¶</strong>ï¼šGit å’Œ GitHub ä½¿ç”¨</li>\n</ul>\n<h3>1.3 æœºå™¨å­¦ä¹ åŸºç¡€</h3>\n<h4>æ ¸å¿ƒæ¦‚å¿µç†è§£</h4>\n<pre><code>ç›‘ç£å­¦ä¹  vs æ— ç›‘ç£å­¦ä¹ \nâ”œâ”€â”€ åˆ†ç±»é—®é¢˜ï¼ˆClassificationï¼‰\nâ”œâ”€â”€ å›å½’é—®é¢˜ï¼ˆRegressionï¼‰\nâ”œâ”€â”€ èšç±»ï¼ˆClusteringï¼‰\nâ””â”€â”€ é™ç»´ï¼ˆDimensionality Reductionï¼‰\n\næ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–\nâ”œâ”€â”€ äº¤å‰éªŒè¯ï¼ˆCross Validationï¼‰\nâ”œâ”€â”€ è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ\nâ”œâ”€â”€ æ­£åˆ™åŒ–æŠ€æœ¯\nâ””â”€â”€ è¶…å‚æ•°è°ƒä¼˜\n</code></pre>\n<h4>å®è·µé¡¹ç›®</h4>\n<ol>\n<li><strong>æ–‡æœ¬åˆ†ç±»é¡¹ç›®</strong>ï¼šä½¿ç”¨ä¼ ç»Ÿ ML æ–¹æ³•è¿›è¡Œæƒ…æ„Ÿåˆ†æ</li>\n<li><strong>æ¨èç³»ç»Ÿ</strong>ï¼šåŸºäºååŒè¿‡æ»¤çš„ç”µå½±æ¨è</li>\n<li><strong>æ•°æ®æŒ–æ˜</strong>ï¼šæ–°é—»æ–‡æœ¬èšç±»åˆ†æ</li>\n</ol>\n<hr>\n<h2>ğŸ§  ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦å­¦ä¹ ä¸ NLPï¼ˆ3-6ä¸ªæœˆï¼‰</h2>\n<h3>2.1 æ·±åº¦å­¦ä¹ åŸºç¡€</h3>\n<h4>ç¥ç»ç½‘ç»œæ¶æ„æ¼”è¿›</h4>\n<pre><code>ç¥ç»ç½‘ç»œå‘å±•å†ç¨‹\nâ”œâ”€â”€ æ„ŸçŸ¥æœºï¼ˆPerceptronï¼‰\nâ”œâ”€â”€ å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰\nâ”œâ”€â”€ å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰\nâ”œâ”€â”€ å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNN/LSTM/GRUï¼‰\nâ””â”€â”€ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰\n</code></pre>\n<h4>PyTorch å®æˆ˜</h4>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# æ¨¡å‹è®­ç»ƒç¤ºä¾‹\nmodel = SimpleNN(784, 128, 10)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n</code></pre>\n<h3>2.2 è‡ªç„¶è¯­è¨€å¤„ç†åŸºç¡€</h3>\n<h4>NLP æ ¸å¿ƒä»»åŠ¡</h4>\n<pre><code>æ–‡æœ¬é¢„å¤„ç†\nâ”œâ”€â”€ åˆ†è¯ï¼ˆTokenizationï¼‰\nâ”œâ”€â”€ è¯æ€§æ ‡æ³¨ï¼ˆPOS Taggingï¼‰\nâ”œâ”€â”€ å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰\nâ””â”€â”€ å¥æ³•åˆ†æï¼ˆParsingï¼‰\n\næ–‡æœ¬è¡¨ç¤ºæ–¹æ³•\nâ”œâ”€â”€ è¯è¢‹æ¨¡å‹ï¼ˆBag of Wordsï¼‰\nâ”œâ”€â”€ TF-IDF\nâ”œâ”€â”€ Word2Vec\nâ”œâ”€â”€ GloVe\nâ””â”€â”€ FastText\n</code></pre>\n<h4>å®è·µé¡¹ç›®</h4>\n<ol>\n<li><strong>è¯å‘é‡è®­ç»ƒ</strong>ï¼šä½¿ç”¨ Word2Vec è®­ç»ƒä¸­æ–‡è¯å‘é‡</li>\n<li><strong>æ–‡æœ¬ç›¸ä¼¼åº¦</strong>ï¼šåŸºäºè¯å‘é‡çš„æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—</li>\n<li><strong>åºåˆ—æ ‡æ³¨</strong>ï¼šä½¿ç”¨ LSTM è¿›è¡Œå‘½åå®ä½“è¯†åˆ«</li>\n</ol>\n<h3>2.3 æ³¨æ„åŠ›æœºåˆ¶æ·±å…¥</h3>\n<h4>Attention æœºåˆ¶ç†è§£</h4>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, hidden_size):\n        super(AttentionLayer, self).__init__()\n        self.hidden_size = hidden_size\n        self.W = nn.Linear(hidden_size, hidden_size)\n        \n    def forward(self, query, key, value):\n        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / (self.hidden_size ** 0.5)\n        \n        # åº”ç”¨ softmax\n        attention_weights = F.softmax(scores, dim=-1)\n        \n        # åŠ æƒæ±‚å’Œ\n        output = torch.matmul(attention_weights, value)\n        return output, attention_weights\n</code></pre>\n<hr>\n<h2>ğŸš€ ç¬¬ä¸‰é˜¶æ®µï¼šTransformer ä¸ LLM æ ¸å¿ƒï¼ˆ6-10ä¸ªæœˆï¼‰</h2>\n<h3>3.1 Transformer æ¶æ„æ·±åº¦è§£æ</h3>\n<h4>æ ¸å¿ƒç»„ä»¶ç†è§£</h4>\n<pre><code>Transformer æ¶æ„\nâ”œâ”€â”€ Multi-Head Attention\nâ”‚   â”œâ”€â”€ Self-Attention æœºåˆ¶\nâ”‚   â”œâ”€â”€ Queryã€Keyã€Value çŸ©é˜µ\nâ”‚   â””â”€â”€ å¤šå¤´æ³¨æ„åŠ›å¹¶è¡Œè®¡ç®—\nâ”œâ”€â”€ Position Encoding\nâ”‚   â”œâ”€â”€ ç»å¯¹ä½ç½®ç¼–ç \nâ”‚   â””â”€â”€ ç›¸å¯¹ä½ç½®ç¼–ç \nâ”œâ”€â”€ Feed Forward Network\nâ””â”€â”€ Layer Normalization\n</code></pre>\n<h4>ä»é›¶å®ç° Transformer</h4>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n            \n        attention_weights = torch.softmax(scores, dim=-1)\n        output = torch.matmul(attention_weights, V)\n        return output, attention_weights\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # çº¿æ€§å˜æ¢å¹¶é‡å¡‘ä¸ºå¤šå¤´\n        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # åº”ç”¨æ³¨æ„åŠ›\n        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # é‡å¡‘å¹¶åº”ç”¨è¾“å‡ºæŠ•å½±\n        attention_output = attention_output.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.d_model)\n        output = self.W_o(attention_output)\n        \n        return output, attention_weights\n</code></pre>\n<h3>3.2 é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹</h3>\n<h4>æ¨¡å‹æ¶æ„å¯¹æ¯”</h4>\n<pre><code>GPT ç³»åˆ—ï¼ˆç”Ÿæˆå¼ï¼‰\nâ”œâ”€â”€ GPT-1: 117M å‚æ•°\nâ”œâ”€â”€ GPT-2: 1.5B å‚æ•°\nâ”œâ”€â”€ GPT-3: 175B å‚æ•°\nâ””â”€â”€ GPT-4: å‚æ•°é‡æœªå…¬å¼€\n\nBERT ç³»åˆ—ï¼ˆç†è§£å¼ï¼‰\nâ”œâ”€â”€ BERT-Base: 110M å‚æ•°\nâ”œâ”€â”€ BERT-Large: 340M å‚æ•°\nâ””â”€â”€ RoBERTa: BERT çš„æ”¹è¿›ç‰ˆæœ¬\n\nT5 ç³»åˆ—ï¼ˆç¼–ç -è§£ç ï¼‰\nâ”œâ”€â”€ T5-Small: 60M å‚æ•°\nâ”œâ”€â”€ T5-Base: 220M å‚æ•°\nâ””â”€â”€ T5-Large: 770M å‚æ•°\n</code></pre>\n<h4>ä½¿ç”¨ Hugging Face Transformers</h4>\n<pre><code class=\"language-python\">from transformers import (\n    AutoTokenizer, \n    AutoModel, \n    AutoModelForCausalLM,\n    pipeline\n)\n\n# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# æ–‡æœ¬ç”Ÿæˆ\ngenerator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nresult = generator(\"äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•\", max_length=100, num_return_sequences=1)\nprint(result[0]['generated_text'])\n\n# è‡ªå®šä¹‰æ¨ç†\ninput_text = \"æœºå™¨å­¦ä¹ æ˜¯\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    output = model.generate(\n        input_ids,\n        max_length=50,\n        num_return_sequences=1,\n        temperature=0.7,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(generated_text)\n</code></pre>\n<h3>3.3 æ¨¡å‹å¾®è°ƒæŠ€æœ¯</h3>\n<h4>Fine-tuning ç­–ç•¥</h4>\n<pre><code>å¾®è°ƒæ–¹æ³•åˆ†ç±»\nâ”œâ”€â”€ å…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-tuningï¼‰\nâ”œâ”€â”€ å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰\nâ”‚   â”œâ”€â”€ LoRAï¼ˆLow-Rank Adaptationï¼‰\nâ”‚   â”œâ”€â”€ Adapter Tuning\nâ”‚   â”œâ”€â”€ Prefix Tuning\nâ”‚   â””â”€â”€ P-Tuning v2\nâ””â”€â”€ æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰\n</code></pre>\n<h4>LoRA å¾®è°ƒå®ç°</h4>\n<pre><code class=\"language-python\">from peft import LoraConfig, get_peft_model, TaskType\n\n# é…ç½® LoRA\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=8,  # rank\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\n# åº”ç”¨ LoRA åˆ°æ¨¡å‹\nmodel = get_peft_model(model, lora_config)\n\n# è®­ç»ƒå¾ªç¯\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        optimizer.zero_grad()\n        \n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        \n        optimizer.step()\n        \n        if step % 100 == 0:\n            print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item()}\")\n</code></pre>\n<hr>\n<h2>ğŸ› ï¸ ç¬¬å››é˜¶æ®µï¼šå®æˆ˜é¡¹ç›®ä¸åº”ç”¨ï¼ˆ10-15ä¸ªæœˆï¼‰</h2>\n<h3>4.1 LLM åº”ç”¨å¼€å‘</h3>\n<h4>é¡¹ç›®ä¸€ï¼šæ™ºèƒ½é—®ç­”ç³»ç»Ÿ</h4>\n<pre><code class=\"language-python\">import openai\nfrom langchain import OpenAI, PromptTemplate, LLMChain\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import TextLoader\n\nclass IntelligentQA:\n    def __init__(self, api_key):\n        self.llm = OpenAI(openai_api_key=api_key)\n        self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n        self.vectorstore = None\n        \n    def build_knowledge_base(self, documents):\n        \"\"\"æ„å»ºçŸ¥è¯†åº“\"\"\"\n        self.vectorstore = FAISS.from_documents(documents, self.embeddings)\n        \n    def answer_question(self, question):\n        \"\"\"å›ç­”é—®é¢˜\"\"\"\n        if not self.vectorstore:\n            return \"çŸ¥è¯†åº“æœªåˆå§‹åŒ–\"\n            \n        # æ£€ç´¢ç›¸å…³æ–‡æ¡£\n        docs = self.vectorstore.similarity_search(question, k=3)\n        context = \"\\n\".join([doc.page_content for doc in docs])\n        \n        # æ„å»ºæç¤ºæ¨¡æ¿\n        template = \"\"\"\n        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n        \n        ä¸Šä¸‹æ–‡ï¼š{context}\n        \n        é—®é¢˜ï¼š{question}\n        \n        ç­”æ¡ˆï¼š\n        \"\"\"\n        \n        prompt = PromptTemplate(\n            template=template,\n            input_variables=[\"context\", \"question\"]\n        )\n        \n        chain = LLMChain(llm=self.llm, prompt=prompt)\n        response = chain.run(context=context, question=question)\n        \n        return response\n\n# ä½¿ç”¨ç¤ºä¾‹\nqa_system = IntelligentQA(\"your-api-key\")\n# åŠ è½½æ–‡æ¡£å¹¶æ„å»ºçŸ¥è¯†åº“\n# qa_system.build_knowledge_base(documents)\n# answer = qa_system.answer_question(\"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\")\n</code></pre>\n<h4>é¡¹ç›®äºŒï¼šä»£ç ç”ŸæˆåŠ©æ‰‹</h4>\n<pre><code class=\"language-python\">from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nclass CodeGenerator:\n    def __init__(self, model_name=\"microsoft/CodeGPT-small-py\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        \n    def generate_code(self, prompt, max_length=200):\n        \"\"\"ç”Ÿæˆä»£ç \"\"\"\n        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs,\n                max_length=max_length,\n                num_return_sequences=1,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n        \n        generated_code = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return generated_code[len(prompt):]\n    \n    def explain_code(self, code):\n        \"\"\"è§£é‡Šä»£ç \"\"\"\n        prompt = f\"è¯·è§£é‡Šä»¥ä¸‹ä»£ç çš„åŠŸèƒ½ï¼š\\n{code}\\nè§£é‡Šï¼š\"\n        return self.generate_code(prompt)\n\n# ä½¿ç”¨ç¤ºä¾‹\ncode_gen = CodeGenerator()\nprompt = \"# å®ç°å¿«é€Ÿæ’åºç®—æ³•\\ndef quicksort(arr):\"\ngenerated = code_gen.generate_code(prompt)\nprint(generated)\n</code></pre>\n<h3>4.2 æ¨¡å‹éƒ¨ç½²ä¸ä¼˜åŒ–</h3>\n<h4>æ¨¡å‹é‡åŒ–</h4>\n<pre><code class=\"language-python\">import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndef quantize_model(model_path, output_path):\n    \"\"\"æ¨¡å‹é‡åŒ–\"\"\"\n    # åŠ è½½æ¨¡å‹\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    \n    # åŠ¨æ€é‡åŒ–\n    quantized_model = torch.quantization.quantize_dynamic(\n        model, \n        {torch.nn.Linear}, \n        dtype=torch.qint8\n    )\n    \n    # ä¿å­˜é‡åŒ–æ¨¡å‹\n    torch.save(quantized_model.state_dict(), output_path)\n    \n    return quantized_model\n\n# æ¨¡å‹æ¨ç†ä¼˜åŒ–\nclass OptimizedInference:\n    def __init__(self, model_path):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForCausalLM.from_pretrained(model_path)\n        \n        # å¯ç”¨æ¨ç†ä¼˜åŒ–\n        self.model.eval()\n        if torch.cuda.is_available():\n            self.model = self.model.cuda()\n            \n    @torch.no_grad()\n    def generate(self, prompt, **kwargs):\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n            \n        outputs = self.model.generate(**inputs, **kwargs)\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n</code></pre>\n<hr>\n<h2>ğŸ“ ç¬¬äº”é˜¶æ®µï¼šé«˜çº§æŠ€æœ¯ä¸ç ”ç©¶ï¼ˆ15ä¸ªæœˆ+ï¼‰</h2>\n<h3>5.1 å¤šæ¨¡æ€å¤§æ¨¡å‹</h3>\n<h4>è§†è§‰-è¯­è¨€æ¨¡å‹</h4>\n<pre><code class=\"language-python\">from transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\n\nclass MultimodalModel:\n    def __init__(self):\n        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    \n    def image_to_text(self, image_path):\n        \"\"\"å›¾åƒæè¿°ç”Ÿæˆ\"\"\"\n        image = Image.open(image_path)\n        inputs = self.processor(image, return_tensors=\"pt\")\n        \n        out = self.model.generate(**inputs, max_length=50)\n        caption = self.processor.decode(out[0], skip_special_tokens=True)\n        \n        return caption\n    \n    def visual_question_answering(self, image_path, question):\n        \"\"\"è§†è§‰é—®ç­”\"\"\"\n        image = Image.open(image_path)\n        inputs = self.processor(image, question, return_tensors=\"pt\")\n        \n        out = self.model.generate(**inputs, max_length=50)\n        answer = self.processor.decode(out[0], skip_special_tokens=True)\n        \n        return answer\n</code></pre>\n<h3>5.2 å¼ºåŒ–å­¦ä¹ ä¸ RLHF</h3>\n<h4>äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ </h4>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass RLHFTrainer:\n    def __init__(self, model_name):\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.reward_model = self.build_reward_model()\n        \n    def build_reward_model(self):\n        \"\"\"æ„å»ºå¥–åŠ±æ¨¡å‹\"\"\"\n        class RewardModel(nn.Module):\n            def __init__(self, base_model):\n                super().__init__()\n                self.base_model = base_model\n                self.reward_head = nn.Linear(base_model.config.hidden_size, 1)\n                \n            def forward(self, input_ids, attention_mask=None):\n                outputs = self.base_model(input_ids, attention_mask=attention_mask)\n                rewards = self.reward_head(outputs.last_hidden_state)\n                return rewards.squeeze(-1)\n        \n        return RewardModel(self.model)\n    \n    def ppo_step(self, prompts, responses, rewards):\n        \"\"\"PPO è®­ç»ƒæ­¥éª¤\"\"\"\n        # è®¡ç®—ç­–ç•¥æ¢¯åº¦\n        # å®ç° PPO ç®—æ³•\n        pass\n</code></pre>\n<hr>\n<h2>ğŸ“ˆ å­¦ä¹ èµ„æºæ¨è</h2>\n<h3>ğŸ“– å¿…è¯»ä¹¦ç±</h3>\n<ol>\n<li><strong>ã€Šæ·±åº¦å­¦ä¹ ã€‹</strong> - Ian Goodfellow</li>\n<li><strong>ã€Šè‡ªç„¶è¯­è¨€å¤„ç†ç»¼è®ºã€‹</strong> - Daniel Jurafsky</li>\n<li><strong>ã€ŠAttention Is All You Needã€‹</strong> - Transformer åŸè®ºæ–‡</li>\n<li><strong>ã€ŠLanguage Models are Few-Shot Learnersã€‹</strong> - GPT-3 è®ºæ–‡</li>\n</ol>\n<h3>ğŸ¥ åœ¨çº¿è¯¾ç¨‹</h3>\n<ol>\n<li><strong>CS224N: Natural Language Processing with Deep Learning</strong> (Stanford)</li>\n<li><strong>CS231N: Convolutional Neural Networks</strong> (Stanford)</li>\n<li><strong>Fast.ai Deep Learning Course</strong></li>\n<li><strong>Hugging Face Course</strong></li>\n</ol>\n<h3>ğŸ› ï¸ å®è·µå¹³å°</h3>\n<ol>\n<li><strong>Hugging Face Hub</strong> - æ¨¡å‹å’Œæ•°æ®é›†</li>\n<li><strong>Google Colab</strong> - å…è´¹ GPU è®­ç»ƒ</li>\n<li><strong>Kaggle</strong> - ç«èµ›å’Œæ•°æ®é›†</li>\n<li><strong>Papers With Code</strong> - è®ºæ–‡å’Œä»£ç </li>\n</ol>\n<h3>ğŸŒ ç¤¾åŒºèµ„æº</h3>\n<ol>\n<li><strong>GitHub</strong> - å¼€æºé¡¹ç›®å’Œä»£ç </li>\n<li><strong>Reddit r/MachineLearning</strong> - å­¦æœ¯è®¨è®º</li>\n<li><strong>Twitter</strong> - æœ€æ–°ç ”ç©¶åŠ¨æ€</li>\n<li><strong>çŸ¥ä¹/CSDN</strong> - ä¸­æ–‡æŠ€æœ¯ç¤¾åŒº</li>\n</ol>\n<hr>\n<h2>ğŸš€ èŒä¸šå‘å±•è·¯å¾„</h2>\n<h3>æŠ€æœ¯å²—ä½</h3>\n<ul>\n<li><strong>AI å·¥ç¨‹å¸ˆ</strong>ï¼šæ¨¡å‹å¼€å‘å’Œéƒ¨ç½²</li>\n<li><strong>ç®—æ³•å·¥ç¨‹å¸ˆ</strong>ï¼šç®—æ³•ç ”ç©¶å’Œä¼˜åŒ–</li>\n<li><strong>æ•°æ®ç§‘å­¦å®¶</strong>ï¼šæ•°æ®åˆ†æå’Œå»ºæ¨¡</li>\n<li><strong>ç ”ç©¶ç§‘å­¦å®¶</strong>ï¼šå‰æ²¿æŠ€æœ¯ç ”ç©¶</li>\n</ul>\n<h3>èƒ½åŠ›è¦æ±‚</h3>\n<ul>\n<li><strong>æŠ€æœ¯æ·±åº¦</strong>ï¼šæ·±å…¥ç†è§£ LLM åŸç†å’Œå®ç°</li>\n<li><strong>å·¥ç¨‹èƒ½åŠ›</strong>ï¼šå¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡å’Œä¼˜åŒ–</li>\n<li><strong>ç ”ç©¶èƒ½åŠ›</strong>ï¼šè·Ÿè¸ªå‰æ²¿æŠ€æœ¯å’Œåˆ›æ–°</li>\n<li><strong>æ²Ÿé€šèƒ½åŠ›</strong>ï¼šæŠ€æœ¯æ–¹æ¡ˆè¡¨è¾¾å’Œå›¢é˜Ÿåä½œ</li>\n</ul>\n<h3>è–ªèµ„æ°´å¹³ï¼ˆ2024å¹´ï¼‰</h3>\n<ul>\n<li><strong>åˆçº§</strong>ï¼š20-40ä¸‡/å¹´</li>\n<li><strong>ä¸­çº§</strong>ï¼š40-80ä¸‡/å¹´</li>\n<li><strong>é«˜çº§</strong>ï¼š80-150ä¸‡/å¹´</li>\n<li><strong>ä¸“å®¶</strong>ï¼š150ä¸‡+/å¹´</li>\n</ul>\n<hr>\n<h2>ğŸ¯ å­¦ä¹ å»ºè®®ä¸æ€»ç»“</h2>\n<h3>å­¦ä¹ ç­–ç•¥</h3>\n<ol>\n<li><strong>ç†è®ºä¸å®è·µå¹¶é‡</strong>ï¼šä¸è¦åªçœ‹è®ºæ–‡ï¼Œè¦åŠ¨æ‰‹å®ç°</li>\n<li><strong>å¾ªåºæ¸è¿›</strong>ï¼šä»ç®€å•æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥æ·±å…¥</li>\n<li><strong>é¡¹ç›®é©±åŠ¨</strong>ï¼šé€šè¿‡å®é™…é¡¹ç›®å·©å›ºçŸ¥è¯†</li>\n<li><strong>æŒç»­å­¦ä¹ </strong>ï¼šå…³æ³¨æœ€æ–°ç ”ç©¶å’ŒæŠ€æœ¯å‘å±•</li>\n</ol>\n<h3>å¸¸è§è¯¯åŒº</h3>\n<ul>\n<li>âŒ æ€¥äºæ±‚æˆï¼Œè·³è¿‡åŸºç¡€çŸ¥è¯†</li>\n<li>âŒ åªå…³æ³¨æœ€æ–°æŠ€æœ¯ï¼Œå¿½è§†åŸºç¡€åŸç†</li>\n<li>âŒ çº¸ä¸Šè°ˆå…µï¼Œç¼ºä¹å®é™…ç¼–ç¨‹ç»éªŒ</li>\n<li>âŒ å­¤å†›å¥‹æˆ˜ï¼Œä¸å‚ä¸æŠ€æœ¯ç¤¾åŒº</li>\n</ul>\n<h3>æˆåŠŸè¦ç´ </h3>\n<ul>\n<li>âœ… æ‰å®çš„æ•°å­¦å’Œç¼–ç¨‹åŸºç¡€</li>\n<li>âœ… æŒç»­çš„å­¦ä¹ å’Œå®è·µ</li>\n<li>âœ… ç§¯æçš„æŠ€æœ¯ç¤¾åŒºå‚ä¸</li>\n<li>âœ… æ¸…æ™°çš„èŒä¸šè§„åˆ’å’Œç›®æ ‡</li>\n</ul>\n<p>LLM æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œè¿™æ˜¯ä¸€ä¸ªå……æ»¡æœºé‡çš„é¢†åŸŸã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„å­¦ä¹ å’ŒæŒç»­çš„å®è·µï¼Œæ‚¨ä¸€å®šèƒ½å¤Ÿåœ¨è¿™ä¸ªæ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸä¸­å–å¾—æˆåŠŸï¼ğŸš€</p>\n<p>è®°ä½ï¼š<strong>å­¦ä¹  LLM ä¸æ˜¯ç»ˆç‚¹ï¼Œè€Œæ˜¯å¼€å¯ AI æ—¶ä»£çš„èµ·ç‚¹</strong>ã€‚ä¿æŒå¥½å¥‡å¿ƒï¼ŒæŒç»­å­¦ä¹ ï¼Œæ‹¥æŠ±å˜åŒ–ï¼Œæ‚¨å°†åœ¨è¿™ä¸ªé¢†åŸŸä¸­æ‰¾åˆ°å±äºè‡ªå·±çš„ä½ç½®ã€‚</p>"
  },
  "_id": "articles/llm-learning-roadmap.md",
  "_raw": {
    "sourceFilePath": "articles/llm-learning-roadmap.md",
    "sourceFileName": "llm-learning-roadmap.md",
    "sourceFileDir": "articles",
    "contentType": "markdown",
    "flattenedPath": "articles/llm-learning-roadmap"
  },
  "type": "Article",
  "slug": "llm-learning-roadmap",
  "readingTime": {
    "text": "14 min read",
    "minutes": 13.94,
    "time": 836400,
    "words": 2788
  },
  "url": "/articles/llm-learning-roadmap"
}
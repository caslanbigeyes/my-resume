{
  "title": "机器学习03 - AIE54 Day03学习笔记",
  "date": "2024-06-17T00:00:00.000Z",
  "summary": "机器学习第三天学习内容，包括数据预处理、线性回归、逻辑回归、模型评估和模型保存",
  "tags": [
    "机器学习",
    "线性回归",
    "逻辑回归",
    "数据预处理",
    "模型评估",
    "AIE54"
  ],
  "readingTime": 18,
  "hasImages": true,
  "slug": "2024-06-17",
  "body": {
    "raw": "\n# 机器学习03 - AIE54 Day03学习笔记\n\n> **学习日期**：2024-06-17  \n> **主讲老师**：李晓华  \n> **课时**：2  \n> **文档来源**：day03-机器学习03.pdf\n\n## 课程关键词\n机器学习 | 数据预处理 | 线性回归 | 逻辑回归 | 模型评估 | KNN算法\n\n---\n\n## 🎯 第一部分：机器学习基础概念\n\n### 1.1 有监督学习流程\n\n**标准机器学习流程**包括以下关键步骤：\n\n1. **数据收集与加载**\n2. **特征工程与数据预处理**\n3. **模型选择与训练**\n4. **模型评估与优化**\n5. **模型部署与保存**\n\n### 1.2 回归 vs 分类\n\n**核心区别**：\n\n- **回归问题**：预测连续数值（如房价预测、温度预测）\n- **分类问题**：预测离散类别（如疾病诊断、图像识别）\n\n**应用场景**：\n- 回归：股价预测、销量预测、年龄估计\n- 分类：垃圾邮件检测、情感分析、医疗诊断\n\n---\n\n## 🔧 第二部分：数据预处理与标准化\n\n### 2.1 数据标准化的重要性\n\n**核心目的**：消除不同特征之间量纲差异的影响，确保所有特征在相同尺度上参与模型训练。\n\n**为什么需要标准化？**\n- 不同特征的数值范围可能相差巨大\n- 大数值特征会主导模型学习过程\n- 影响基于距离的算法（如KNN、SVM）\n\n### 2.2 标准化公式\n\n**Z-score 标准化公式**：\n\nX_normalized = (X - μ) / (σ + ε)\n\n其中：\n- μ 是均值\n- σ 是标准差  \n- ε = 1e-9 是防止除零的小常数\n\n### 2.3 代码实现\n\n```python\n# 计算训练集的均值和标准差\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\n\n# 对训练集和测试集进行标准化\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n```\n\n**⚠️ 重要注意事项**：\n- 测试集必须使用训练集的均值和标准差进行标准化\n- 避免数据泄露问题\n- 保证模型在真实环境中的泛化能力\n\n---\n\n## 📈 第三部分：线性回归\n\n### 3.1 理论基础\n\n**线性回归假设**：目标变量与特征变量之间存在线性关系\n\n**数学表达式**：\ny = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ + ε\n\n其中：\n- y 是目标变量\n- wᵢ 是权重参数\n- xᵢ 是特征变量\n- ε 是误差项\n\n### 3.2 损失函数\n\n**均方误差（MSE）**作为损失函数：\n\nMSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²\n\n**特点**：\n- 对异常值敏感\n- 可导，便于优化\n- 几何意义明确\n\n### 3.3 完整代码示例\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# 1. 数据加载\ndata = pd.read_csv(\"boston_house_prices.csv\", skiprows=1)\n\n# 2. 特征和标签分离\nX = data.loc[:, :\"LSTAT\"].to_numpy()  # 特征\ny = data.loc[:, \"MEDV\"].to_numpy()    # 标签\n\n# 3. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n# 4. 数据标准化\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n\n# 5. 模型训练\nlr = LinearRegression()\nlr.fit(X=X_train, y=y_train)\n\n# 6. 预测与评估\ny_pred = lr.predict(X=X_test)\nmse = ((y_pred - y_test) ** 2).mean()\nprint(f\"均方误差: {mse}\")\n```\n\n---\n\n## 🧠 第四部分：逻辑回归与分类\n\n### 4.1 Sigmoid 激活函数\n\n**核心作用**：将线性输出映射到 (0,1) 区间，表示概率\n\n**数学公式**：\nσ(z) = 1 / (1 + e^(-z))\n\n### 4.2 Sigmoid 函数特性\n\n**重要特性**：\n- **输出范围**：(0, 1)\n- **S型曲线**：平滑的概率转换\n- **导数易计算**：σ'(z) = σ(z)(1 - σ(z))\n- **单调递增**：适合概率建模\n\n### 4.3 代码实现\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    \"\"\"Sigmoid 激活函数\"\"\"\n    return 1 / (1 + np.exp(-x))\n\n# 可视化 Sigmoid 函数\nx = np.linspace(-10, 10, 100)\nplt.plot(x, sigmoid(x))\nplt.grid()\nplt.title('Sigmoid Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.show()\n```\n\n### 4.4 逻辑回归完整流程\n\n```python\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# 1. 加载数据\nX, y = load_breast_cancer(return_X_y=True)\n\n# 2. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n# 3. 数据标准化\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n\n# 4. 模型训练\nlr = LogisticRegression()\nlr.fit(X=X_train, y=y_train)\n\n# 5. 预测与评估\ny_pred = lr.predict(X=X_test)\naccuracy = (y_pred == y_test).mean()\nprint(f\"准确率: {accuracy:.4f}\")\n\n# 查看模型参数\nprint(f\"权重数量: {lr.coef_.shape}\")\nprint(f\"偏置项: {lr.intercept_}\")\n```\n\n---\n\n## 📊 第五部分：模型评估\n\n### 5.1 回归问题评估指标\n\n**均方误差 (MSE)**：\n\nMSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²\n\n```python\nmse = ((y_pred - y_test) ** 2).mean()\n```\n\n**其他常用指标**：\n- **MAE**：平均绝对误差\n- **RMSE**：均方根误差\n- **R²**：决定系数\n\n### 5.2 分类问题评估指标\n\n**准确率 (Accuracy)**：\n\nAccuracy = 正确预测数量 / 总预测数量\n\n```python\naccuracy = (y_pred == y_test).mean()\n```\n\n**其他重要指标**：\n- **精确率 (Precision)**\n- **召回率 (Recall)**\n- **F1-Score**\n- **AUC-ROC**\n\n---\n\n## 💾 第六部分：模型保存与加载\n\n### 6.1 序列化概念\n\n**核心概念**：\n- **序列化**：将内存中的对象转化为字节流，保存到硬盘\n- **反序列化**：将硬盘上的文件读入，转化为内存中的对象\n\n### 6.2 Python 序列化工具对比\n\n| 工具 | 特点 | 适用场景 |\n|------|------|----------|\n| **pickle** | 底层，操作相对复杂 | 通用Python对象序列化 |\n| **joblib** | 上层，操作简便 | 专为大型NumPy数组和科学计算优化 |\n\n### 6.3 代码示例\n\n**使用 pickle 保存单个模型**：\n\n```python\nimport pickle\n\n# 保存模型\nwith open(\"lr.pickle\", \"wb\") as f:\n    pickle.dump(obj=lr, file=f)\n\n# 加载模型\nwith open(\"lr.pickle\", \"rb\") as f:\n    lr_loaded = pickle.load(file=f)\n```\n\n**使用 joblib 保存多个模型**：\n\n```python\nimport joblib\n\n# 保存多个模型\njoblib.dump(value=knn, filename=\"knn.joblib\")\njoblib.dump(value=[lr, knn], filename=\"models.joblib\")\n\n# 加载模型\nknn_loaded = joblib.load(filename=\"knn.joblib\")\nlr_loaded, knn_loaded = joblib.load(filename=\"models.joblib\")\n```\n\n---\n\n## 📐 第七部分：统计学概念：方差\n\n### 7.1 方差的两种估计方法\n\n**1. 总体方差（有偏估计）**：\n\nσ² = (1/N) × Σᵢ₌₁ᴺ (xᵢ - μ)²\n\n**2. 样本方差（无偏估计）**：\n\ns² = (1/(N-1)) × Σᵢ₌₁ᴺ (xᵢ - x̄)²\n\n### 7.2 库函数差异\n\n| 库 | 默认方法 | 参数控制 |\n|-----|----------|----------|\n| **NumPy** | 有偏估计 (除以N) | ddof=1 使用无偏估计 |\n| **PyTorch** | 无偏估计 (除以N-1) | correction=1 控制 |\n\n### 7.3 代码示例\n\n```python\nimport numpy as np\nimport torch\n\nls = [1, 2, 3, 4, 5, 6]\n\n# NumPy - 默认有偏估计\narr = np.array(ls)\nbiased_std = arr.std()        # 有偏\nunbiased_std = arr.std(ddof=1)  # 无偏\n\n# PyTorch - 默认无偏估计\nt = torch.tensor(ls, dtype=torch.float32)\nunbiased_std = t.std(correction=1)  # 无偏\n```\n\n### 7.4 注意事项\n\n1. **防止除零错误**：\n```python\nsigma = X_train.std(axis=0) + 1e-9\n```\n\n2. **选择合适的估计方法**：\n   - 样本量较小时，使用无偏估计更准确\n   - 深度学习中，通常使用有偏估计（计算效率更高）\n\n---\n\n## 🚀 第八部分：KNN算法补充\n\n### 8.1 K近邻算法特点\n\n**核心特性**：\n- **非参数方法**：不需要假设数据分布\n- **懒惰学习**：训练阶段只存储数据，预测时才计算\n- **适用性广**：回归和分类问题都适用\n\n### 8.2 代码示例\n\n```python\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n\n# 回归任务\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X=X_train, y=y_train)\n\n# 分类任务\nknn_clf = KNeighborsClassifier(n_neighbors=5)\nknn_clf.fit(X=X_train, y=y_train)\n```\n\n---\n\n## 🎯 关键要点总结\n\n### ✅ 最佳实践\n\n1. **数据预处理必不可少**：标准化能显著提升模型性能\n2. **合理划分数据集**：训练集用于学习，测试集用于评估\n3. **选择合适的评估指标**：MSE用于回归，准确率用于分类\n4. **模型保存很重要**：使用joblib保存训练好的模型\n\n### 🔍 深入理解\n\n1. **线性回归**：适用于线性关系明显的回归问题\n2. **逻辑回归**：通过Sigmoid函数实现分类，本质上是线性分类器\n3. **方差估计**：理解有偏与无偏的区别，根据场景选择\n\n### 📈 进阶方向\n\n- **正则化方法**（Ridge、Lasso）\n- **模型选择与交叉验证**\n- **特征工程技巧**\n- **集成学习方法**\n\n---\n\n## 学习心得\n\n这节课深入学习了机器学习的核心算法和实践技巧：\n\n1. **数据预处理的重要性**：标准化不仅是技术要求，更是保证模型公平性的关键步骤\n2. **线性模型的威力**：虽然简单，但线性回归和逻辑回归在很多实际问题中都表现优异\n3. **模型评估的科学性**：选择合适的评估指标对模型性能判断至关重要\n4. **工程实践的价值**：模型保存、加载等工程技巧是实际部署的必备技能\n\n## 下一步学习计划\n- 深入学习正则化技术\n- 探索更复杂的非线性模型\n- 实践特征工程技巧\n- 学习模型调优方法\n",
    "html": "<h1>机器学习03 - AIE54 Day03学习笔记</h1>\n<blockquote>\n<p><strong>学习日期</strong>：2024-06-17<br>\n<strong>主讲老师</strong>：李晓华<br>\n<strong>课时</strong>：2<br>\n<strong>文档来源</strong>：day03-机器学习03.pdf</p>\n</blockquote>\n<h2>课程关键词</h2>\n<p>机器学习 | 数据预处理 | 线性回归 | 逻辑回归 | 模型评估 | KNN算法</p>\n<hr>\n<h2>🎯 第一部分：机器学习基础概念</h2>\n<h3>1.1 有监督学习流程</h3>\n<p><strong>标准机器学习流程</strong>包括以下关键步骤：</p>\n<ol>\n<li><strong>数据收集与加载</strong></li>\n<li><strong>特征工程与数据预处理</strong></li>\n<li><strong>模型选择与训练</strong></li>\n<li><strong>模型评估与优化</strong></li>\n<li><strong>模型部署与保存</strong></li>\n</ol>\n<h3>1.2 回归 vs 分类</h3>\n<p><strong>核心区别</strong>：</p>\n<ul>\n<li><strong>回归问题</strong>：预测连续数值（如房价预测、温度预测）</li>\n<li><strong>分类问题</strong>：预测离散类别（如疾病诊断、图像识别）</li>\n</ul>\n<p><strong>应用场景</strong>：</p>\n<ul>\n<li>回归：股价预测、销量预测、年龄估计</li>\n<li>分类：垃圾邮件检测、情感分析、医疗诊断</li>\n</ul>\n<hr>\n<h2>🔧 第二部分：数据预处理与标准化</h2>\n<h3>2.1 数据标准化的重要性</h3>\n<p><strong>核心目的</strong>：消除不同特征之间量纲差异的影响，确保所有特征在相同尺度上参与模型训练。</p>\n<p><strong>为什么需要标准化？</strong></p>\n<ul>\n<li>不同特征的数值范围可能相差巨大</li>\n<li>大数值特征会主导模型学习过程</li>\n<li>影响基于距离的算法（如KNN、SVM）</li>\n</ul>\n<h3>2.2 标准化公式</h3>\n<p><strong>Z-score 标准化公式</strong>：</p>\n<p>X_normalized = (X - μ) / (σ + ε)</p>\n<p>其中：</p>\n<ul>\n<li>μ 是均值</li>\n<li>σ 是标准差</li>\n<li>ε = 1e-9 是防止除零的小常数</li>\n</ul>\n<h3>2.3 代码实现</h3>\n<pre><code class=\"language-python\"># 计算训练集的均值和标准差\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\n\n# 对训练集和测试集进行标准化\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n</code></pre>\n<p><strong>⚠️ 重要注意事项</strong>：</p>\n<ul>\n<li>测试集必须使用训练集的均值和标准差进行标准化</li>\n<li>避免数据泄露问题</li>\n<li>保证模型在真实环境中的泛化能力</li>\n</ul>\n<hr>\n<h2>📈 第三部分：线性回归</h2>\n<h3>3.1 理论基础</h3>\n<p><strong>线性回归假设</strong>：目标变量与特征变量之间存在线性关系</p>\n<p><strong>数学表达式</strong>：\ny = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ + ε</p>\n<p>其中：</p>\n<ul>\n<li>y 是目标变量</li>\n<li>wᵢ 是权重参数</li>\n<li>xᵢ 是特征变量</li>\n<li>ε 是误差项</li>\n</ul>\n<h3>3.2 损失函数</h3>\n<p>**均方误差（MSE）**作为损失函数：</p>\n<p>MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²</p>\n<p><strong>特点</strong>：</p>\n<ul>\n<li>对异常值敏感</li>\n<li>可导，便于优化</li>\n<li>几何意义明确</li>\n</ul>\n<h3>3.3 完整代码示例</h3>\n<pre><code class=\"language-python\">import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# 1. 数据加载\ndata = pd.read_csv(\"boston_house_prices.csv\", skiprows=1)\n\n# 2. 特征和标签分离\nX = data.loc[:, :\"LSTAT\"].to_numpy()  # 特征\ny = data.loc[:, \"MEDV\"].to_numpy()    # 标签\n\n# 3. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n# 4. 数据标准化\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n\n# 5. 模型训练\nlr = LinearRegression()\nlr.fit(X=X_train, y=y_train)\n\n# 6. 预测与评估\ny_pred = lr.predict(X=X_test)\nmse = ((y_pred - y_test) ** 2).mean()\nprint(f\"均方误差: {mse}\")\n</code></pre>\n<hr>\n<h2>🧠 第四部分：逻辑回归与分类</h2>\n<h3>4.1 Sigmoid 激活函数</h3>\n<p><strong>核心作用</strong>：将线性输出映射到 (0,1) 区间，表示概率</p>\n<p><strong>数学公式</strong>：\nσ(z) = 1 / (1 + e^(-z))</p>\n<h3>4.2 Sigmoid 函数特性</h3>\n<p><strong>重要特性</strong>：</p>\n<ul>\n<li><strong>输出范围</strong>：(0, 1)</li>\n<li><strong>S型曲线</strong>：平滑的概率转换</li>\n<li><strong>导数易计算</strong>：σ'(z) = σ(z)(1 - σ(z))</li>\n<li><strong>单调递增</strong>：适合概率建模</li>\n</ul>\n<h3>4.3 代码实现</h3>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    \"\"\"Sigmoid 激活函数\"\"\"\n    return 1 / (1 + np.exp(-x))\n\n# 可视化 Sigmoid 函数\nx = np.linspace(-10, 10, 100)\nplt.plot(x, sigmoid(x))\nplt.grid()\nplt.title('Sigmoid Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.show()\n</code></pre>\n<h3>4.4 逻辑回归完整流程</h3>\n<pre><code class=\"language-python\">from sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# 1. 加载数据\nX, y = load_breast_cancer(return_X_y=True)\n\n# 2. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n# 3. 数据标准化\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n\n# 4. 模型训练\nlr = LogisticRegression()\nlr.fit(X=X_train, y=y_train)\n\n# 5. 预测与评估\ny_pred = lr.predict(X=X_test)\naccuracy = (y_pred == y_test).mean()\nprint(f\"准确率: {accuracy:.4f}\")\n\n# 查看模型参数\nprint(f\"权重数量: {lr.coef_.shape}\")\nprint(f\"偏置项: {lr.intercept_}\")\n</code></pre>\n<hr>\n<h2>📊 第五部分：模型评估</h2>\n<h3>5.1 回归问题评估指标</h3>\n<p><strong>均方误差 (MSE)</strong>：</p>\n<p>MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²</p>\n<pre><code class=\"language-python\">mse = ((y_pred - y_test) ** 2).mean()\n</code></pre>\n<p><strong>其他常用指标</strong>：</p>\n<ul>\n<li><strong>MAE</strong>：平均绝对误差</li>\n<li><strong>RMSE</strong>：均方根误差</li>\n<li><strong>R²</strong>：决定系数</li>\n</ul>\n<h3>5.2 分类问题评估指标</h3>\n<p><strong>准确率 (Accuracy)</strong>：</p>\n<p>Accuracy = 正确预测数量 / 总预测数量</p>\n<pre><code class=\"language-python\">accuracy = (y_pred == y_test).mean()\n</code></pre>\n<p><strong>其他重要指标</strong>：</p>\n<ul>\n<li><strong>精确率 (Precision)</strong></li>\n<li><strong>召回率 (Recall)</strong></li>\n<li><strong>F1-Score</strong></li>\n<li><strong>AUC-ROC</strong></li>\n</ul>\n<hr>\n<h2>💾 第六部分：模型保存与加载</h2>\n<h3>6.1 序列化概念</h3>\n<p><strong>核心概念</strong>：</p>\n<ul>\n<li><strong>序列化</strong>：将内存中的对象转化为字节流，保存到硬盘</li>\n<li><strong>反序列化</strong>：将硬盘上的文件读入，转化为内存中的对象</li>\n</ul>\n<h3>6.2 Python 序列化工具对比</h3>\n<p>| 工具 | 特点 | 适用场景 |\n|------|------|----------|\n| <strong>pickle</strong> | 底层，操作相对复杂 | 通用Python对象序列化 |\n| <strong>joblib</strong> | 上层，操作简便 | 专为大型NumPy数组和科学计算优化 |</p>\n<h3>6.3 代码示例</h3>\n<p><strong>使用 pickle 保存单个模型</strong>：</p>\n<pre><code class=\"language-python\">import pickle\n\n# 保存模型\nwith open(\"lr.pickle\", \"wb\") as f:\n    pickle.dump(obj=lr, file=f)\n\n# 加载模型\nwith open(\"lr.pickle\", \"rb\") as f:\n    lr_loaded = pickle.load(file=f)\n</code></pre>\n<p><strong>使用 joblib 保存多个模型</strong>：</p>\n<pre><code class=\"language-python\">import joblib\n\n# 保存多个模型\njoblib.dump(value=knn, filename=\"knn.joblib\")\njoblib.dump(value=[lr, knn], filename=\"models.joblib\")\n\n# 加载模型\nknn_loaded = joblib.load(filename=\"knn.joblib\")\nlr_loaded, knn_loaded = joblib.load(filename=\"models.joblib\")\n</code></pre>\n<hr>\n<h2>📐 第七部分：统计学概念：方差</h2>\n<h3>7.1 方差的两种估计方法</h3>\n<p><strong>1. 总体方差（有偏估计）</strong>：</p>\n<p>σ² = (1/N) × Σᵢ₌₁ᴺ (xᵢ - μ)²</p>\n<p><strong>2. 样本方差（无偏估计）</strong>：</p>\n<p>s² = (1/(N-1)) × Σᵢ₌₁ᴺ (xᵢ - x̄)²</p>\n<h3>7.2 库函数差异</h3>\n<p>| 库 | 默认方法 | 参数控制 |\n|-----|----------|----------|\n| <strong>NumPy</strong> | 有偏估计 (除以N) | ddof=1 使用无偏估计 |\n| <strong>PyTorch</strong> | 无偏估计 (除以N-1) | correction=1 控制 |</p>\n<h3>7.3 代码示例</h3>\n<pre><code class=\"language-python\">import numpy as np\nimport torch\n\nls = [1, 2, 3, 4, 5, 6]\n\n# NumPy - 默认有偏估计\narr = np.array(ls)\nbiased_std = arr.std()        # 有偏\nunbiased_std = arr.std(ddof=1)  # 无偏\n\n# PyTorch - 默认无偏估计\nt = torch.tensor(ls, dtype=torch.float32)\nunbiased_std = t.std(correction=1)  # 无偏\n</code></pre>\n<h3>7.4 注意事项</h3>\n<ol>\n<li><strong>防止除零错误</strong>：</li>\n</ol>\n<pre><code class=\"language-python\">sigma = X_train.std(axis=0) + 1e-9\n</code></pre>\n<ol start=\"2\">\n<li><strong>选择合适的估计方法</strong>：\n<ul>\n<li>样本量较小时，使用无偏估计更准确</li>\n<li>深度学习中，通常使用有偏估计（计算效率更高）</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2>🚀 第八部分：KNN算法补充</h2>\n<h3>8.1 K近邻算法特点</h3>\n<p><strong>核心特性</strong>：</p>\n<ul>\n<li><strong>非参数方法</strong>：不需要假设数据分布</li>\n<li><strong>懒惰学习</strong>：训练阶段只存储数据，预测时才计算</li>\n<li><strong>适用性广</strong>：回归和分类问题都适用</li>\n</ul>\n<h3>8.2 代码示例</h3>\n<pre><code class=\"language-python\">from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n\n# 回归任务\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X=X_train, y=y_train)\n\n# 分类任务\nknn_clf = KNeighborsClassifier(n_neighbors=5)\nknn_clf.fit(X=X_train, y=y_train)\n</code></pre>\n<hr>\n<h2>🎯 关键要点总结</h2>\n<h3>✅ 最佳实践</h3>\n<ol>\n<li><strong>数据预处理必不可少</strong>：标准化能显著提升模型性能</li>\n<li><strong>合理划分数据集</strong>：训练集用于学习，测试集用于评估</li>\n<li><strong>选择合适的评估指标</strong>：MSE用于回归，准确率用于分类</li>\n<li><strong>模型保存很重要</strong>：使用joblib保存训练好的模型</li>\n</ol>\n<h3>🔍 深入理解</h3>\n<ol>\n<li><strong>线性回归</strong>：适用于线性关系明显的回归问题</li>\n<li><strong>逻辑回归</strong>：通过Sigmoid函数实现分类，本质上是线性分类器</li>\n<li><strong>方差估计</strong>：理解有偏与无偏的区别，根据场景选择</li>\n</ol>\n<h3>📈 进阶方向</h3>\n<ul>\n<li><strong>正则化方法</strong>（Ridge、Lasso）</li>\n<li><strong>模型选择与交叉验证</strong></li>\n<li><strong>特征工程技巧</strong></li>\n<li><strong>集成学习方法</strong></li>\n</ul>\n<hr>\n<h2>学习心得</h2>\n<p>这节课深入学习了机器学习的核心算法和实践技巧：</p>\n<ol>\n<li><strong>数据预处理的重要性</strong>：标准化不仅是技术要求，更是保证模型公平性的关键步骤</li>\n<li><strong>线性模型的威力</strong>：虽然简单，但线性回归和逻辑回归在很多实际问题中都表现优异</li>\n<li><strong>模型评估的科学性</strong>：选择合适的评估指标对模型性能判断至关重要</li>\n<li><strong>工程实践的价值</strong>：模型保存、加载等工程技巧是实际部署的必备技能</li>\n</ol>\n<h2>下一步学习计划</h2>\n<ul>\n<li>深入学习正则化技术</li>\n<li>探索更复杂的非线性模型</li>\n<li>实践特征工程技巧</li>\n<li>学习模型调优方法</li>\n</ul>"
  },
  "_id": "learning-notes/2024-06-17.md",
  "_raw": {
    "sourceFilePath": "learning-notes/2024-06-17.md",
    "sourceFileName": "2024-06-17.md",
    "sourceFileDir": "learning-notes",
    "contentType": "markdown",
    "flattenedPath": "learning-notes/2024-06-17"
  },
  "type": "LearningNote",
  "url": "/learning-notes/2024-06-17"
}
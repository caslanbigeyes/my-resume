{
  "title": "机器学习02 - AIE54 Day02学习笔记",
  "date": "2024-06-14T00:00:00.000Z",
  "summary": "机器学习第二天学习内容，包括NumPy基本操作、向量化操作、特征工程和K近邻算法",
  "tags": [
    "机器学习",
    "NumPy",
    "向量化编程",
    "KNN",
    "特征工程",
    "AIE54"
  ],
  "readingTime": 15,
  "hasImages": true,
  "slug": "2024-06-14-getting-started",
  "body": {
    "raw": "\n# 机器学习02 - AIE54 Day02学习笔记\n\n> **学习日期**：2024-06-14  \n> **主讲老师**：李晓华  \n> **课时**：6  \n> **文档来源**：AIE54_day02-机器学习02.pdf\n\n## 课程关键词\nNumPy | 向量化操作 | 广播机制 | 特征工程 | K近邻算法 | 标准化\n\n---\n\n## 🔢 第一部分：NumPy 基本操作\n\n### 1.1 数组创建\n\n**核心概念**：NumPy 是 Python 科学计算的基础库，提供高效的多维数组对象和数学函数。\n\n```python\nimport numpy as np\n\n# 从列表创建数组\nscores = [random.randint(0, 100) for _ in range(300)]\narr = np.array(scores)\n\n# 创建特殊数组\nzeros_array = np.zeros(shape=(2, 3, 4, 5))  # 全零数组\nones_array = np.ones(shape=(2,))            # 全一数组\nrange_array = np.arange(10)                 # 0-9的数组\nlinspace_array = np.linspace(-10, 10, 100) # 等差数列\n```\n\n**重要属性**：\n- `arr.shape` : 数组形状\n- `arr.size` : 元素总数\n- `arr.dtype` : 数据类型\n\n### 1.2 元素级运算（Element-wise Operations）\n\n**关键特性**：NumPy 支持向量化操作，自动应用到每个元素上。\n\n```python\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# 基本运算\nresult_add = a + b    # [5, 7, 9]\nresult_mul = a * b    # [4, 10, 18]\nresult_pow = a ** b   # [1, 32, 729]\n\n# 与标量运算\nscalar_add = a + 1    # [2, 3, 4]\n```\n\n**与 Python 列表对比**：\n- 列表：`[1,2,3] + [4,5,6] = [1,2,3,4,5,6]` (连接)\n- NumPy：`array([1,2,3]) + array([4,5,6]) = array([5,7,9])` (元素级相加)\n\n### 1.3 广播机制（Broadcasting）\n\n**定义**：在不引起歧义的前提下，NumPy 自动调整数组形状以进行运算。\n\n```python\narr1 = np.array([1, 2, 3, 4, 5])  # shape: (5,)\narr2 = np.array([3])              # shape: (1,)\nresult = arr1 + arr2              # 广播成功: [4, 5, 6, 7, 8]\n\n# 二维广播示例\narr2d = np.array([[1], [2]])      # shape: (2, 1)\nresult2d = arr1 + arr2d           # 结果 shape: (2, 5)\n```\n\n**广播规则**：\n1. 从最后一个维度开始比较\n2. 维度大小相等或其中一个为 1 时可以广播\n3. 缺失维度视为 1\n\n### 1.4 数学函数\n\n```python\narr = np.array([1, 2, 3, 4, 5])\n\n# 三角函数\nsin_values = np.sin(arr)\ncos_values = np.cos(arr)\n\n# 指数和对数\nexp_values = np.exp(arr)\nlog_values = np.log(arr)\nlog2_values = np.log2(arr)\nlog10_values = np.log10(arr)\n```\n\n---\n\n## 📐 第二部分：NumPy 向量化操作\n\n### 2.1 向量空间基础\n**核心概念**：从向量空间角度理解数据，涉及模长、夹角和内积。\n\n### 2.2 向量模长计算\n\n**数学公式**：对于向量 v = [v₁, v₂, ..., vₙ]，模长为：\n\n|v| = √(v₁² + v₂² + ... + vₙ²)\n\n```python\nv1 = np.array([1, 2, 3, 4, 5])\n\n# 方法1：手动计算\nnorm_manual = (v1 ** 2).sum() ** 0.5\n\n# 方法2：使用 NumPy 函数\nnorm_numpy = np.linalg.norm(v1)\n```\n\n### 2.3 向量内积与余弦相似度\n\n**内积公式**：v₁ · v₂ = Σᵢ₌₁ⁿ v₁ᵢ × v₂ᵢ\n\n**余弦相似度公式**：cos(θ) = (v₁ · v₂) / (|v₁| × |v₂|)\n\n```python\nv1 = np.array([1, 2, 3, 4, 5])\nv2 = np.array([6, 3, 1, 3, 6])\n\n# 内积计算\ndot_product_manual = (v1 * v2).sum()\ndot_product_operator = v1 @ v2\n\n# 余弦相似度\ncosine_similarity = v1 @ v2 / (np.linalg.norm(v1) * np.linalg.norm(v2))\n```\n\n### 2.4 欧几里得距离\n\n**公式**：d(v₁, v₂) = √(Σᵢ₌₁ⁿ (v₁ᵢ - v₂ᵢ)²)\n\n```python\n# 欧几里得距离\neuclidean_distance = ((v1 - v2) ** 2).sum() ** 0.5\n# 或使用 NumPy\neuclidean_distance = np.linalg.norm(v1 - v2)\n```\n\n### 2.5 矩阵运算\n\n```python\nm1 = np.arange(12).reshape(3, 4)  # 3×4 矩阵\nm2 = np.arange(12).reshape(4, 3)  # 4×3 矩阵\n\n# 矩阵乘法（注意维度匹配）\nresult = m1 @ m2  # 结果为 3×3 矩阵\n```\n\n---\n\n## 🔧 第三部分：特征工程\n\n### 3.1 数据集介绍\n\n**使用乳腺癌威斯康辛数据集**：\n- 样本数量：569 个\n- 特征数量：30 个数值特征\n- 目标变量：二分类（恶性/良性）\n\n```python\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\nprint(f\"特征矩阵形状: {X.shape}\")  # (569, 30)\nprint(f\"目标向量形状: {y.shape}\")  # (569,)\n```\n\n### 3.2 特征标准化\n\n**问题**：不同特征的量纲和数值范围差异巨大，会影响算法性能。\n\n**解决方案**：Z-score 标准化\n\n**数学公式**：x_标准化 = (x - μ) / σ\n\n其中：\n- μ 是特征均值\n- σ 是特征标准差\n\n```python\n# 计算统计量\nmu = X.mean(axis=0)      # 各特征均值\nsigma = X.std(axis=0)    # 各特征标准差\n\n# 标准化\nX_standardized = (X - mu) / sigma\n```\n\n**标准化效果**：\n- 均值变为 0\n- 标准差变为 1\n- 消除量纲影响\n\n### 3.3 其他规范化方法\n\n**Min-Max 规范化**：\n\n```python\n# Min-Max 规范化到 [0,1] 区间\nX_minmax = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n```\n\n---\n\n## 🎯 第四部分：K 近邻分类算法\n\n### 4.1 算法原理\n\n**KNN 核心思想**：根据样本在特征空间中的 K 个最近邻居的标签来预测其类别。\n\n**决策规则**：对于分类问题，采用多数投票原则。\n\n### 4.2 完整机器学习流程\n\n```python\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# 1. 加载数据\nX, y = load_breast_cancer(return_X_y=True)\n\n# 2. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, shuffle=True, random_state=0\n)\n\n# 3. 特征标准化（重要！）\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0)\nX_train_scaled = (X_train - mu) / sigma\nX_test_scaled = (X_test - mu) / sigma\n\n# 4. 模型训练\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n\n# 5. 预测和评估\ny_pred = knn.predict(X_test_scaled)\naccuracy = (y_pred == y_test).mean()\nprint(f\"准确率: {accuracy:.4f}\")\n```\n\n### 4.3 关键注意事项\n\n**为什么需要标准化？**\n- KNN 基于距离计算\n- 不同特征的数值范围差异会导致某些特征主导距离计算\n- 标准化确保所有特征等权重参与计算\n\n**数据泄露防范**：\n- ✅ **正确做法**：用训练集统计量标准化测试集\n- ❌ **错误做法**：用全体数据统计量标准化\n\n---\n\n## 📈 第五部分：K 近邻回归算法\n\n### 5.1 数据集介绍\n\n**使用波士顿房价数据集**：\n- 样本数量：506 个\n- 特征数量：13 个房屋相关特征\n- 目标变量：房价（连续值）\n\n```python\nimport pandas as pd\n\n# 加载数据\ndata = pd.read_csv(\"boston_house_prices.csv\", skiprows=1)\nX = data.loc[:, :\"LSTAT\"].to_numpy()  # 特征\ny = data.loc[:, \"MEDV\"].to_numpy()    # 目标变量（房价）\n```\n\n### 5.2 回归算法流程\n\n```python\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# 数据切分和标准化（与分类相同）\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0)\nX_train_scaled = (X_train - mu) / sigma\nX_test_scaled = (X_test - mu) / sigma\n\n# 回归模型\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X_train_scaled, y_train)\ny_pred = knn_reg.predict(X_test_scaled)\n```\n\n### 5.3 回归评估指标\n\n**平均绝对误差 (MAE)**：MAE = (1/n) × Σᵢ₌₁ⁿ |yᵢ - ŷᵢ|\n\n**均方误差 (MSE)**：MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²\n\n```python\n# 评估指标计算\nmae = abs(y_pred - y_test).mean()\nmse = ((y_pred - y_test) ** 2).mean()\nrmse = mse ** 0.5\n\nprint(f\"平均绝对误差: {mae:.4f}\")\nprint(f\"均方误差: {mse:.4f}\")\nprint(f\"均方根误差: {rmse:.4f}\")\n```\n\n---\n\n## 🔑 核心要点总结\n\n### NumPy 核心概念\n- **向量化操作**：提高计算效率的关键\n- **广播机制**：实现不同形状数组的运算\n- **线性代数函数**：模长、内积、距离计算\n\n### 特征工程要点\n- **标准化必要性**：消除特征间的量纲差异\n- **数据泄露防范**：始终用训练集统计量处理测试集\n- **Z-score vs Min-Max**：根据数据分布选择合适方法\n\n### KNN 算法核心\n- **距离度量**：欧几里得距离最常用\n- **K 值选择**：影响模型复杂度和性能\n- **分类 vs 回归**：投票机制 vs 平均值机制\n\n### 机器学习标准流程\n1. **数据加载** → 2. **数据切分** → 3. **特征预处理** → 4. **模型训练** → 5. **预测评估**\n\n---\n\n## 🚀 实践建议\n\n1. **代码实践**：每个概念都要亲自编写代码验证\n2. **参数调优**：尝试不同的 K 值，观察性能变化\n3. **数据可视化**：用图表理解数据分布和算法效果\n4. **扩展学习**：了解其他距离度量方法和预处理技术\n\n---\n\n## 学习心得\n\n这节课系统地学习了机器学习的基础工具和算法：\n\n1. **NumPy的重要性**：作为科学计算基础，向量化操作大大提升了计算效率\n2. **特征工程的关键性**：数据预处理直接影响模型性能，标准化是必不可少的步骤\n3. **KNN算法的简洁性**：虽然简单，但在很多场景下都很有效\n4. **机器学习流程的规范性**：严格按照标准流程可以避免很多常见错误\n\n## 下一步学习计划\n- 06-17\n\n",
    "html": "<h1>机器学习02 - AIE54 Day02学习笔记</h1>\n<blockquote>\n<p><strong>学习日期</strong>：2024-06-14<br>\n<strong>主讲老师</strong>：李晓华<br>\n<strong>课时</strong>：6<br>\n<strong>文档来源</strong>：AIE54_day02-机器学习02.pdf</p>\n</blockquote>\n<h2>课程关键词</h2>\n<p>NumPy | 向量化操作 | 广播机制 | 特征工程 | K近邻算法 | 标准化</p>\n<hr>\n<h2>🔢 第一部分：NumPy 基本操作</h2>\n<h3>1.1 数组创建</h3>\n<p><strong>核心概念</strong>：NumPy 是 Python 科学计算的基础库，提供高效的多维数组对象和数学函数。</p>\n<pre><code class=\"language-python\">import numpy as np\n\n# 从列表创建数组\nscores = [random.randint(0, 100) for _ in range(300)]\narr = np.array(scores)\n\n# 创建特殊数组\nzeros_array = np.zeros(shape=(2, 3, 4, 5))  # 全零数组\nones_array = np.ones(shape=(2,))            # 全一数组\nrange_array = np.arange(10)                 # 0-9的数组\nlinspace_array = np.linspace(-10, 10, 100) # 等差数列\n</code></pre>\n<p><strong>重要属性</strong>：</p>\n<ul>\n<li><code>arr.shape</code> : 数组形状</li>\n<li><code>arr.size</code> : 元素总数</li>\n<li><code>arr.dtype</code> : 数据类型</li>\n</ul>\n<h3>1.2 元素级运算（Element-wise Operations）</h3>\n<p><strong>关键特性</strong>：NumPy 支持向量化操作，自动应用到每个元素上。</p>\n<pre><code class=\"language-python\">a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# 基本运算\nresult_add = a + b    # [5, 7, 9]\nresult_mul = a * b    # [4, 10, 18]\nresult_pow = a ** b   # [1, 32, 729]\n\n# 与标量运算\nscalar_add = a + 1    # [2, 3, 4]\n</code></pre>\n<p><strong>与 Python 列表对比</strong>：</p>\n<ul>\n<li>列表：<code>[1,2,3] + [4,5,6] = [1,2,3,4,5,6]</code> (连接)</li>\n<li>NumPy：<code>array([1,2,3]) + array([4,5,6]) = array([5,7,9])</code> (元素级相加)</li>\n</ul>\n<h3>1.3 广播机制（Broadcasting）</h3>\n<p><strong>定义</strong>：在不引起歧义的前提下，NumPy 自动调整数组形状以进行运算。</p>\n<pre><code class=\"language-python\">arr1 = np.array([1, 2, 3, 4, 5])  # shape: (5,)\narr2 = np.array([3])              # shape: (1,)\nresult = arr1 + arr2              # 广播成功: [4, 5, 6, 7, 8]\n\n# 二维广播示例\narr2d = np.array([[1], [2]])      # shape: (2, 1)\nresult2d = arr1 + arr2d           # 结果 shape: (2, 5)\n</code></pre>\n<p><strong>广播规则</strong>：</p>\n<ol>\n<li>从最后一个维度开始比较</li>\n<li>维度大小相等或其中一个为 1 时可以广播</li>\n<li>缺失维度视为 1</li>\n</ol>\n<h3>1.4 数学函数</h3>\n<pre><code class=\"language-python\">arr = np.array([1, 2, 3, 4, 5])\n\n# 三角函数\nsin_values = np.sin(arr)\ncos_values = np.cos(arr)\n\n# 指数和对数\nexp_values = np.exp(arr)\nlog_values = np.log(arr)\nlog2_values = np.log2(arr)\nlog10_values = np.log10(arr)\n</code></pre>\n<hr>\n<h2>📐 第二部分：NumPy 向量化操作</h2>\n<h3>2.1 向量空间基础</h3>\n<p><strong>核心概念</strong>：从向量空间角度理解数据，涉及模长、夹角和内积。</p>\n<h3>2.2 向量模长计算</h3>\n<p><strong>数学公式</strong>：对于向量 v = [v₁, v₂, ..., vₙ]，模长为：</p>\n<p>|v| = √(v₁² + v₂² + ... + vₙ²)</p>\n<pre><code class=\"language-python\">v1 = np.array([1, 2, 3, 4, 5])\n\n# 方法1：手动计算\nnorm_manual = (v1 ** 2).sum() ** 0.5\n\n# 方法2：使用 NumPy 函数\nnorm_numpy = np.linalg.norm(v1)\n</code></pre>\n<h3>2.3 向量内积与余弦相似度</h3>\n<p><strong>内积公式</strong>：v₁ · v₂ = Σᵢ₌₁ⁿ v₁ᵢ × v₂ᵢ</p>\n<p><strong>余弦相似度公式</strong>：cos(θ) = (v₁ · v₂) / (|v₁| × |v₂|)</p>\n<pre><code class=\"language-python\">v1 = np.array([1, 2, 3, 4, 5])\nv2 = np.array([6, 3, 1, 3, 6])\n\n# 内积计算\ndot_product_manual = (v1 * v2).sum()\ndot_product_operator = v1 @ v2\n\n# 余弦相似度\ncosine_similarity = v1 @ v2 / (np.linalg.norm(v1) * np.linalg.norm(v2))\n</code></pre>\n<h3>2.4 欧几里得距离</h3>\n<p><strong>公式</strong>：d(v₁, v₂) = √(Σᵢ₌₁ⁿ (v₁ᵢ - v₂ᵢ)²)</p>\n<pre><code class=\"language-python\"># 欧几里得距离\neuclidean_distance = ((v1 - v2) ** 2).sum() ** 0.5\n# 或使用 NumPy\neuclidean_distance = np.linalg.norm(v1 - v2)\n</code></pre>\n<h3>2.5 矩阵运算</h3>\n<pre><code class=\"language-python\">m1 = np.arange(12).reshape(3, 4)  # 3×4 矩阵\nm2 = np.arange(12).reshape(4, 3)  # 4×3 矩阵\n\n# 矩阵乘法（注意维度匹配）\nresult = m1 @ m2  # 结果为 3×3 矩阵\n</code></pre>\n<hr>\n<h2>🔧 第三部分：特征工程</h2>\n<h3>3.1 数据集介绍</h3>\n<p><strong>使用乳腺癌威斯康辛数据集</strong>：</p>\n<ul>\n<li>样本数量：569 个</li>\n<li>特征数量：30 个数值特征</li>\n<li>目标变量：二分类（恶性/良性）</li>\n</ul>\n<pre><code class=\"language-python\">from sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\nprint(f\"特征矩阵形状: {X.shape}\")  # (569, 30)\nprint(f\"目标向量形状: {y.shape}\")  # (569,)\n</code></pre>\n<h3>3.2 特征标准化</h3>\n<p><strong>问题</strong>：不同特征的量纲和数值范围差异巨大，会影响算法性能。</p>\n<p><strong>解决方案</strong>：Z-score 标准化</p>\n<p><strong>数学公式</strong>：x_标准化 = (x - μ) / σ</p>\n<p>其中：</p>\n<ul>\n<li>μ 是特征均值</li>\n<li>σ 是特征标准差</li>\n</ul>\n<pre><code class=\"language-python\"># 计算统计量\nmu = X.mean(axis=0)      # 各特征均值\nsigma = X.std(axis=0)    # 各特征标准差\n\n# 标准化\nX_standardized = (X - mu) / sigma\n</code></pre>\n<p><strong>标准化效果</strong>：</p>\n<ul>\n<li>均值变为 0</li>\n<li>标准差变为 1</li>\n<li>消除量纲影响</li>\n</ul>\n<h3>3.3 其他规范化方法</h3>\n<p><strong>Min-Max 规范化</strong>：</p>\n<pre><code class=\"language-python\"># Min-Max 规范化到 [0,1] 区间\nX_minmax = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n</code></pre>\n<hr>\n<h2>🎯 第四部分：K 近邻分类算法</h2>\n<h3>4.1 算法原理</h3>\n<p><strong>KNN 核心思想</strong>：根据样本在特征空间中的 K 个最近邻居的标签来预测其类别。</p>\n<p><strong>决策规则</strong>：对于分类问题，采用多数投票原则。</p>\n<h3>4.2 完整机器学习流程</h3>\n<pre><code class=\"language-python\">from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# 1. 加载数据\nX, y = load_breast_cancer(return_X_y=True)\n\n# 2. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, shuffle=True, random_state=0\n)\n\n# 3. 特征标准化（重要！）\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0)\nX_train_scaled = (X_train - mu) / sigma\nX_test_scaled = (X_test - mu) / sigma\n\n# 4. 模型训练\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n\n# 5. 预测和评估\ny_pred = knn.predict(X_test_scaled)\naccuracy = (y_pred == y_test).mean()\nprint(f\"准确率: {accuracy:.4f}\")\n</code></pre>\n<h3>4.3 关键注意事项</h3>\n<p><strong>为什么需要标准化？</strong></p>\n<ul>\n<li>KNN 基于距离计算</li>\n<li>不同特征的数值范围差异会导致某些特征主导距离计算</li>\n<li>标准化确保所有特征等权重参与计算</li>\n</ul>\n<p><strong>数据泄露防范</strong>：</p>\n<ul>\n<li>✅ <strong>正确做法</strong>：用训练集统计量标准化测试集</li>\n<li>❌ <strong>错误做法</strong>：用全体数据统计量标准化</li>\n</ul>\n<hr>\n<h2>📈 第五部分：K 近邻回归算法</h2>\n<h3>5.1 数据集介绍</h3>\n<p><strong>使用波士顿房价数据集</strong>：</p>\n<ul>\n<li>样本数量：506 个</li>\n<li>特征数量：13 个房屋相关特征</li>\n<li>目标变量：房价（连续值）</li>\n</ul>\n<pre><code class=\"language-python\">import pandas as pd\n\n# 加载数据\ndata = pd.read_csv(\"boston_house_prices.csv\", skiprows=1)\nX = data.loc[:, :\"LSTAT\"].to_numpy()  # 特征\ny = data.loc[:, \"MEDV\"].to_numpy()    # 目标变量（房价）\n</code></pre>\n<h3>5.2 回归算法流程</h3>\n<pre><code class=\"language-python\">from sklearn.neighbors import KNeighborsRegressor\n\n# 数据切分和标准化（与分类相同）\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0)\nX_train_scaled = (X_train - mu) / sigma\nX_test_scaled = (X_test - mu) / sigma\n\n# 回归模型\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X_train_scaled, y_train)\ny_pred = knn_reg.predict(X_test_scaled)\n</code></pre>\n<h3>5.3 回归评估指标</h3>\n<p><strong>平均绝对误差 (MAE)</strong>：MAE = (1/n) × Σᵢ₌₁ⁿ |yᵢ - ŷᵢ|</p>\n<p><strong>均方误差 (MSE)</strong>：MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²</p>\n<pre><code class=\"language-python\"># 评估指标计算\nmae = abs(y_pred - y_test).mean()\nmse = ((y_pred - y_test) ** 2).mean()\nrmse = mse ** 0.5\n\nprint(f\"平均绝对误差: {mae:.4f}\")\nprint(f\"均方误差: {mse:.4f}\")\nprint(f\"均方根误差: {rmse:.4f}\")\n</code></pre>\n<hr>\n<h2>🔑 核心要点总结</h2>\n<h3>NumPy 核心概念</h3>\n<ul>\n<li><strong>向量化操作</strong>：提高计算效率的关键</li>\n<li><strong>广播机制</strong>：实现不同形状数组的运算</li>\n<li><strong>线性代数函数</strong>：模长、内积、距离计算</li>\n</ul>\n<h3>特征工程要点</h3>\n<ul>\n<li><strong>标准化必要性</strong>：消除特征间的量纲差异</li>\n<li><strong>数据泄露防范</strong>：始终用训练集统计量处理测试集</li>\n<li><strong>Z-score vs Min-Max</strong>：根据数据分布选择合适方法</li>\n</ul>\n<h3>KNN 算法核心</h3>\n<ul>\n<li><strong>距离度量</strong>：欧几里得距离最常用</li>\n<li><strong>K 值选择</strong>：影响模型复杂度和性能</li>\n<li><strong>分类 vs 回归</strong>：投票机制 vs 平均值机制</li>\n</ul>\n<h3>机器学习标准流程</h3>\n<ol>\n<li><strong>数据加载</strong> → 2. <strong>数据切分</strong> → 3. <strong>特征预处理</strong> → 4. <strong>模型训练</strong> → 5. <strong>预测评估</strong></li>\n</ol>\n<hr>\n<h2>🚀 实践建议</h2>\n<ol>\n<li><strong>代码实践</strong>：每个概念都要亲自编写代码验证</li>\n<li><strong>参数调优</strong>：尝试不同的 K 值，观察性能变化</li>\n<li><strong>数据可视化</strong>：用图表理解数据分布和算法效果</li>\n<li><strong>扩展学习</strong>：了解其他距离度量方法和预处理技术</li>\n</ol>\n<hr>\n<h2>学习心得</h2>\n<p>这节课系统地学习了机器学习的基础工具和算法：</p>\n<ol>\n<li><strong>NumPy的重要性</strong>：作为科学计算基础，向量化操作大大提升了计算效率</li>\n<li><strong>特征工程的关键性</strong>：数据预处理直接影响模型性能，标准化是必不可少的步骤</li>\n<li><strong>KNN算法的简洁性</strong>：虽然简单，但在很多场景下都很有效</li>\n<li><strong>机器学习流程的规范性</strong>：严格按照标准流程可以避免很多常见错误</li>\n</ol>\n<h2>下一步学习计划</h2>\n<ul>\n<li>06-17</li>\n</ul>"
  },
  "_id": "learning-notes/2024-06-14-getting-started.md",
  "_raw": {
    "sourceFilePath": "learning-notes/2024-06-14-getting-started.md",
    "sourceFileName": "2024-06-14-getting-started.md",
    "sourceFileDir": "learning-notes",
    "contentType": "markdown",
    "flattenedPath": "learning-notes/2024-06-14-getting-started"
  },
  "type": "LearningNote",
  "url": "/learning-notes/2024-06-14-getting-started"
}
[
  {
    "title": "机器学习02 - AIE54 Day02学习笔记上册",
    "date": "2024-06-14T00:00:00.000Z",
    "summary": "机器学习第二天学习内容，包括NumPy基本操作、向量化操作、特征工程和K近邻算法",
    "tags": [
      "机器学习",
      "NumPy",
      "向量化编程",
      "KNN",
      "特征工程",
      "AIE54"
    ],
    "readingTime": 15,
    "hasImages": true,
    "slug": "2024-06-14-getting-started",
    "body": {
      "raw": "\n# 机器学习02 - AIE54 Day02学习笔记\n\n> **学习日期**：2024-06-14  \n> **主讲老师**：李晓华  \n> **课时**：6  \n> **文档来源**：AIE54_day02-机器学习02.pdf\n\n## 课程关键词\nNumPy | 向量化操作 | 广播机制 | 特征工程 | K近邻算法 | 标准化\n\n---\n\n## 🔢 第一部分：NumPy 基本操作\n\n### 1.1 数组创建\n\n**核心概念**：NumPy 是 Python 科学计算的基础库，提供高效的多维数组对象和数学函数。\n\n```python\nimport numpy as np\n\n# 从列表创建数组\nscores = [random.randint(0, 100) for _ in range(300)]\narr = np.array(scores)\n\n# 创建特殊数组\nzeros_array = np.zeros(shape=(2, 3, 4, 5))  # 全零数组\nones_array = np.ones(shape=(2,))            # 全一数组\nrange_array = np.arange(10)                 # 0-9的数组\nlinspace_array = np.linspace(-10, 10, 100) # 等差数列\n```\n\n**重要属性**：\n- `arr.shape` : 数组形状\n- `arr.size` : 元素总数\n- `arr.dtype` : 数据类型\n\n### 1.2 元素级运算（Element-wise Operations）\n\n**关键特性**：NumPy 支持向量化操作，自动应用到每个元素上。\n\n```python\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# 基本运算\nresult_add = a + b    # [5, 7, 9]\nresult_mul = a * b    # [4, 10, 18]\nresult_pow = a ** b   # [1, 32, 729]\n\n# 与标量运算\nscalar_add = a + 1    # [2, 3, 4]\n```\n\n**与 Python 列表对比**：\n- 列表：`[1,2,3] + [4,5,6] = [1,2,3,4,5,6]` (连接)\n- NumPy：`array([1,2,3]) + array([4,5,6]) = array([5,7,9])` (元素级相加)\n\n### 1.3 广播机制（Broadcasting）\n\n**定义**：在不引起歧义的前提下，NumPy 自动调整数组形状以进行运算。\n\n```python\narr1 = np.array([1, 2, 3, 4, 5])  # shape: (5,)\narr2 = np.array([3])              # shape: (1,)\nresult = arr1 + arr2              # 广播成功: [4, 5, 6, 7, 8]\n\n# 二维广播示例\narr2d = np.array([[1], [2]])      # shape: (2, 1)\nresult2d = arr1 + arr2d           # 结果 shape: (2, 5)\n```\n\n**广播规则**：\n1. 从最后一个维度开始比较\n2. 维度大小相等或其中一个为 1 时可以广播\n3. 缺失维度视为 1\n\n### 1.4 数学函数\n\n```python\narr = np.array([1, 2, 3, 4, 5])\n\n# 三角函数\nsin_values = np.sin(arr)\ncos_values = np.cos(arr)\n\n# 指数和对数\nexp_values = np.exp(arr)\nlog_values = np.log(arr)\nlog2_values = np.log2(arr)\nlog10_values = np.log10(arr)\n```\n\n---\n\n## 📐 第二部分：NumPy 向量化操作\n\n### 2.1 向量空间基础\n**核心概念**：从向量空间角度理解数据，涉及模长、夹角和内积。\n\n### 2.2 向量模长计算\n\n**数学公式**：对于向量 v = [v₁, v₂, ..., vₙ]，模长为：\n\n|v| = √(v₁² + v₂² + ... + vₙ²)\n\n```python\nv1 = np.array([1, 2, 3, 4, 5])\n\n# 方法1：手动计算\nnorm_manual = (v1 ** 2).sum() ** 0.5\n\n# 方法2：使用 NumPy 函数\nnorm_numpy = np.linalg.norm(v1)\n```\n\n### 2.3 向量内积与余弦相似度\n\n**内积公式**：v₁ · v₂ = Σᵢ₌₁ⁿ v₁ᵢ × v₂ᵢ\n\n**余弦相似度公式**：cos(θ) = (v₁ · v₂) / (|v₁| × |v₂|)\n\n```python\nv1 = np.array([1, 2, 3, 4, 5])\nv2 = np.array([6, 3, 1, 3, 6])\n\n# 内积计算\ndot_product_manual = (v1 * v2).sum()\ndot_product_operator = v1 @ v2\n\n# 余弦相似度\ncosine_similarity = v1 @ v2 / (np.linalg.norm(v1) * np.linalg.norm(v2))\n```\n\n### 2.4 欧几里得距离\n\n**公式**：d(v₁, v₂) = √(Σᵢ₌₁ⁿ (v₁ᵢ - v₂ᵢ)²)\n\n```python\n# 欧几里得距离\neuclidean_distance = ((v1 - v2) ** 2).sum() ** 0.5\n# 或使用 NumPy\neuclidean_distance = np.linalg.norm(v1 - v2)\n```\n\n### 2.5 矩阵运算\n\n```python\nm1 = np.arange(12).reshape(3, 4)  # 3×4 矩阵\nm2 = np.arange(12).reshape(4, 3)  # 4×3 矩阵\n\n# 矩阵乘法（注意维度匹配）\nresult = m1 @ m2  # 结果为 3×3 矩阵\n```\n\n---\n\n## 🔧 第三部分：特征工程\n\n### 3.1 数据集介绍\n\n**使用乳腺癌威斯康辛数据集**：\n- 样本数量：569 个\n- 特征数量：30 个数值特征\n- 目标变量：二分类（恶性/良性）\n\n```python\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\nprint(f\"特征矩阵形状: {X.shape}\")  # (569, 30)\nprint(f\"目标向量形状: {y.shape}\")  # (569,)\n```\n\n### 3.2 特征标准化\n\n**问题**：不同特征的量纲和数值范围差异巨大，会影响算法性能。\n\n**解决方案**：Z-score 标准化\n\n**数学公式**：x_标准化 = (x - μ) / σ\n\n其中：\n- μ 是特征均值\n- σ 是特征标准差\n\n```python\n# 计算统计量\nmu = X.mean(axis=0)      # 各特征均值\nsigma = X.std(axis=0)    # 各特征标准差\n\n# 标准化\nX_standardized = (X - mu) / sigma\n```\n\n**标准化效果**：\n- 均值变为 0\n- 标准差变为 1\n- 消除量纲影响\n\n### 3.3 其他规范化方法\n\n**Min-Max 规范化**：\n\n```python\n# Min-Max 规范化到 [0,1] 区间\nX_minmax = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n```\n\n---\n\n## 🎯 第四部分：K 近邻分类算法\n\n### 4.1 算法原理\n\n**KNN 核心思想**：根据样本在特征空间中的 K 个最近邻居的标签来预测其类别。\n\n**决策规则**：对于分类问题，采用多数投票原则。\n\n### 4.2 完整机器学习流程\n\n```python\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# 1. 加载数据\nX, y = load_breast_cancer(return_X_y=True)\n\n# 2. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, shuffle=True, random_state=0\n)\n\n# 3. 特征标准化（重要！）\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0)\nX_train_scaled = (X_train - mu) / sigma\nX_test_scaled = (X_test - mu) / sigma\n\n# 4. 模型训练\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n\n# 5. 预测和评估\ny_pred = knn.predict(X_test_scaled)\naccuracy = (y_pred == y_test).mean()\nprint(f\"准确率: {accuracy:.4f}\")\n```\n\n### 4.3 关键注意事项\n\n**为什么需要标准化？**\n- KNN 基于距离计算\n- 不同特征的数值范围差异会导致某些特征主导距离计算\n- 标准化确保所有特征等权重参与计算\n\n**数据泄露防范**：\n- ✅ **正确做法**：用训练集统计量标准化测试集\n- ❌ **错误做法**：用全体数据统计量标准化\n\n---\n\n## 📈 第五部分：K 近邻回归算法\n\n### 5.1 数据集介绍\n\n**使用波士顿房价数据集**：\n- 样本数量：506 个\n- 特征数量：13 个房屋相关特征\n- 目标变量：房价（连续值）\n\n```python\nimport pandas as pd\n\n# 加载数据\ndata = pd.read_csv(\"boston_house_prices.csv\", skiprows=1)\nX = data.loc[:, :\"LSTAT\"].to_numpy()  # 特征\ny = data.loc[:, \"MEDV\"].to_numpy()    # 目标变量（房价）\n```\n\n### 5.2 回归算法流程\n\n```python\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# 数据切分和标准化（与分类相同）\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0)\nX_train_scaled = (X_train - mu) / sigma\nX_test_scaled = (X_test - mu) / sigma\n\n# 回归模型\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X_train_scaled, y_train)\ny_pred = knn_reg.predict(X_test_scaled)\n```\n\n### 5.3 回归评估指标\n\n**平均绝对误差 (MAE)**：MAE = (1/n) × Σᵢ₌₁ⁿ |yᵢ - ŷᵢ|\n\n**均方误差 (MSE)**：MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²\n\n```python\n# 评估指标计算\nmae = abs(y_pred - y_test).mean()\nmse = ((y_pred - y_test) ** 2).mean()\nrmse = mse ** 0.5\n\nprint(f\"平均绝对误差: {mae:.4f}\")\nprint(f\"均方误差: {mse:.4f}\")\nprint(f\"均方根误差: {rmse:.4f}\")\n```\n\n---\n\n## 🔑 核心要点总结\n\n### NumPy 核心概念\n- **向量化操作**：提高计算效率的关键\n- **广播机制**：实现不同形状数组的运算\n- **线性代数函数**：模长、内积、距离计算\n\n### 特征工程要点\n- **标准化必要性**：消除特征间的量纲差异\n- **数据泄露防范**：始终用训练集统计量处理测试集\n- **Z-score vs Min-Max**：根据数据分布选择合适方法\n\n### KNN 算法核心\n- **距离度量**：欧几里得距离最常用\n- **K 值选择**：影响模型复杂度和性能\n- **分类 vs 回归**：投票机制 vs 平均值机制\n\n### 机器学习标准流程\n1. **数据加载** → 2. **数据切分** → 3. **特征预处理** → 4. **模型训练** → 5. **预测评估**\n\n---\n\n## 🚀 实践建议\n\n1. **代码实践**：每个概念都要亲自编写代码验证\n2. **参数调优**：尝试不同的 K 值，观察性能变化\n3. **数据可视化**：用图表理解数据分布和算法效果\n4. **扩展学习**：了解其他距离度量方法和预处理技术\n\n---\n\n## 学习心得\n\n这节课系统地学习了机器学习的基础工具和算法：\n\n1. **NumPy的重要性**：作为科学计算基础，向量化操作大大提升了计算效率\n2. **特征工程的关键性**：数据预处理直接影响模型性能，标准化是必不可少的步骤\n3. **KNN算法的简洁性**：虽然简单，但在很多场景下都很有效\n4. **机器学习流程的规范性**：严格按照标准流程可以避免很多常见错误\n\n## 下一步学习计划\n- 06-17\n\n",
      "html": "<h1>机器学习02 - AIE54 Day02学习笔记</h1>\n<blockquote>\n<p><strong>学习日期</strong>：2024-06-14<br>\n<strong>主讲老师</strong>：李晓华<br>\n<strong>课时</strong>：6<br>\n<strong>文档来源</strong>：AIE54_day02-机器学习02.pdf</p>\n</blockquote>\n<h2>课程关键词</h2>\n<p>NumPy | 向量化操作 | 广播机制 | 特征工程 | K近邻算法 | 标准化</p>\n<hr>\n<h2>🔢 第一部分：NumPy 基本操作</h2>\n<h3>1.1 数组创建</h3>\n<p><strong>核心概念</strong>：NumPy 是 Python 科学计算的基础库，提供高效的多维数组对象和数学函数。</p>\n<pre><code class=\"language-python\">import numpy as np\n\n# 从列表创建数组\nscores = [random.randint(0, 100) for _ in range(300)]\narr = np.array(scores)\n\n# 创建特殊数组\nzeros_array = np.zeros(shape=(2, 3, 4, 5))  # 全零数组\nones_array = np.ones(shape=(2,))            # 全一数组\nrange_array = np.arange(10)                 # 0-9的数组\nlinspace_array = np.linspace(-10, 10, 100) # 等差数列\n</code></pre>\n<p><strong>重要属性</strong>：</p>\n<ul>\n<li><code>arr.shape</code> : 数组形状</li>\n<li><code>arr.size</code> : 元素总数</li>\n<li><code>arr.dtype</code> : 数据类型</li>\n</ul>\n<h3>1.2 元素级运算（Element-wise Operations）</h3>\n<p><strong>关键特性</strong>：NumPy 支持向量化操作，自动应用到每个元素上。</p>\n<pre><code class=\"language-python\">a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# 基本运算\nresult_add = a + b    # [5, 7, 9]\nresult_mul = a * b    # [4, 10, 18]\nresult_pow = a ** b   # [1, 32, 729]\n\n# 与标量运算\nscalar_add = a + 1    # [2, 3, 4]\n</code></pre>\n<p><strong>与 Python 列表对比</strong>：</p>\n<ul>\n<li>列表：<code>[1,2,3] + [4,5,6] = [1,2,3,4,5,6]</code> (连接)</li>\n<li>NumPy：<code>array([1,2,3]) + array([4,5,6]) = array([5,7,9])</code> (元素级相加)</li>\n</ul>\n<h3>1.3 广播机制（Broadcasting）</h3>\n<p><strong>定义</strong>：在不引起歧义的前提下，NumPy 自动调整数组形状以进行运算。</p>\n<pre><code class=\"language-python\">arr1 = np.array([1, 2, 3, 4, 5])  # shape: (5,)\narr2 = np.array([3])              # shape: (1,)\nresult = arr1 + arr2              # 广播成功: [4, 5, 6, 7, 8]\n\n# 二维广播示例\narr2d = np.array([[1], [2]])      # shape: (2, 1)\nresult2d = arr1 + arr2d           # 结果 shape: (2, 5)\n</code></pre>\n<p><strong>广播规则</strong>：</p>\n<ol>\n<li>从最后一个维度开始比较</li>\n<li>维度大小相等或其中一个为 1 时可以广播</li>\n<li>缺失维度视为 1</li>\n</ol>\n<h3>1.4 数学函数</h3>\n<pre><code class=\"language-python\">arr = np.array([1, 2, 3, 4, 5])\n\n# 三角函数\nsin_values = np.sin(arr)\ncos_values = np.cos(arr)\n\n# 指数和对数\nexp_values = np.exp(arr)\nlog_values = np.log(arr)\nlog2_values = np.log2(arr)\nlog10_values = np.log10(arr)\n</code></pre>\n<hr>\n<h2>📐 第二部分：NumPy 向量化操作</h2>\n<h3>2.1 向量空间基础</h3>\n<p><strong>核心概念</strong>：从向量空间角度理解数据，涉及模长、夹角和内积。</p>\n<h3>2.2 向量模长计算</h3>\n<p><strong>数学公式</strong>：对于向量 v = [v₁, v₂, ..., vₙ]，模长为：</p>\n<p>|v| = √(v₁² + v₂² + ... + vₙ²)</p>\n<pre><code class=\"language-python\">v1 = np.array([1, 2, 3, 4, 5])\n\n# 方法1：手动计算\nnorm_manual = (v1 ** 2).sum() ** 0.5\n\n# 方法2：使用 NumPy 函数\nnorm_numpy = np.linalg.norm(v1)\n</code></pre>\n<h3>2.3 向量内积与余弦相似度</h3>\n<p><strong>内积公式</strong>：v₁ · v₂ = Σᵢ₌₁ⁿ v₁ᵢ × v₂ᵢ</p>\n<p><strong>余弦相似度公式</strong>：cos(θ) = (v₁ · v₂) / (|v₁| × |v₂|)</p>\n<pre><code class=\"language-python\">v1 = np.array([1, 2, 3, 4, 5])\nv2 = np.array([6, 3, 1, 3, 6])\n\n# 内积计算\ndot_product_manual = (v1 * v2).sum()\ndot_product_operator = v1 @ v2\n\n# 余弦相似度\ncosine_similarity = v1 @ v2 / (np.linalg.norm(v1) * np.linalg.norm(v2))\n</code></pre>\n<h3>2.4 欧几里得距离</h3>\n<p><strong>公式</strong>：d(v₁, v₂) = √(Σᵢ₌₁ⁿ (v₁ᵢ - v₂ᵢ)²)</p>\n<pre><code class=\"language-python\"># 欧几里得距离\neuclidean_distance = ((v1 - v2) ** 2).sum() ** 0.5\n# 或使用 NumPy\neuclidean_distance = np.linalg.norm(v1 - v2)\n</code></pre>\n<h3>2.5 矩阵运算</h3>\n<pre><code class=\"language-python\">m1 = np.arange(12).reshape(3, 4)  # 3×4 矩阵\nm2 = np.arange(12).reshape(4, 3)  # 4×3 矩阵\n\n# 矩阵乘法（注意维度匹配）\nresult = m1 @ m2  # 结果为 3×3 矩阵\n</code></pre>\n<hr>\n<h2>🔧 第三部分：特征工程</h2>\n<h3>3.1 数据集介绍</h3>\n<p><strong>使用乳腺癌威斯康辛数据集</strong>：</p>\n<ul>\n<li>样本数量：569 个</li>\n<li>特征数量：30 个数值特征</li>\n<li>目标变量：二分类（恶性/良性）</li>\n</ul>\n<pre><code class=\"language-python\">from sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\nprint(f\"特征矩阵形状: {X.shape}\")  # (569, 30)\nprint(f\"目标向量形状: {y.shape}\")  # (569,)\n</code></pre>\n<h3>3.2 特征标准化</h3>\n<p><strong>问题</strong>：不同特征的量纲和数值范围差异巨大，会影响算法性能。</p>\n<p><strong>解决方案</strong>：Z-score 标准化</p>\n<p><strong>数学公式</strong>：x_标准化 = (x - μ) / σ</p>\n<p>其中：</p>\n<ul>\n<li>μ 是特征均值</li>\n<li>σ 是特征标准差</li>\n</ul>\n<pre><code class=\"language-python\"># 计算统计量\nmu = X.mean(axis=0)      # 各特征均值\nsigma = X.std(axis=0)    # 各特征标准差\n\n# 标准化\nX_standardized = (X - mu) / sigma\n</code></pre>\n<p><strong>标准化效果</strong>：</p>\n<ul>\n<li>均值变为 0</li>\n<li>标准差变为 1</li>\n<li>消除量纲影响</li>\n</ul>\n<h3>3.3 其他规范化方法</h3>\n<p><strong>Min-Max 规范化</strong>：</p>\n<pre><code class=\"language-python\"># Min-Max 规范化到 [0,1] 区间\nX_minmax = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n</code></pre>\n<hr>\n<h2>🎯 第四部分：K 近邻分类算法</h2>\n<h3>4.1 算法原理</h3>\n<p><strong>KNN 核心思想</strong>：根据样本在特征空间中的 K 个最近邻居的标签来预测其类别。</p>\n<p><strong>决策规则</strong>：对于分类问题，采用多数投票原则。</p>\n<h3>4.2 完整机器学习流程</h3>\n<pre><code class=\"language-python\">from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# 1. 加载数据\nX, y = load_breast_cancer(return_X_y=True)\n\n# 2. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, shuffle=True, random_state=0\n)\n\n# 3. 特征标准化（重要！）\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0)\nX_train_scaled = (X_train - mu) / sigma\nX_test_scaled = (X_test - mu) / sigma\n\n# 4. 模型训练\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n\n# 5. 预测和评估\ny_pred = knn.predict(X_test_scaled)\naccuracy = (y_pred == y_test).mean()\nprint(f\"准确率: {accuracy:.4f}\")\n</code></pre>\n<h3>4.3 关键注意事项</h3>\n<p><strong>为什么需要标准化？</strong></p>\n<ul>\n<li>KNN 基于距离计算</li>\n<li>不同特征的数值范围差异会导致某些特征主导距离计算</li>\n<li>标准化确保所有特征等权重参与计算</li>\n</ul>\n<p><strong>数据泄露防范</strong>：</p>\n<ul>\n<li>✅ <strong>正确做法</strong>：用训练集统计量标准化测试集</li>\n<li>❌ <strong>错误做法</strong>：用全体数据统计量标准化</li>\n</ul>\n<hr>\n<h2>📈 第五部分：K 近邻回归算法</h2>\n<h3>5.1 数据集介绍</h3>\n<p><strong>使用波士顿房价数据集</strong>：</p>\n<ul>\n<li>样本数量：506 个</li>\n<li>特征数量：13 个房屋相关特征</li>\n<li>目标变量：房价（连续值）</li>\n</ul>\n<pre><code class=\"language-python\">import pandas as pd\n\n# 加载数据\ndata = pd.read_csv(\"boston_house_prices.csv\", skiprows=1)\nX = data.loc[:, :\"LSTAT\"].to_numpy()  # 特征\ny = data.loc[:, \"MEDV\"].to_numpy()    # 目标变量（房价）\n</code></pre>\n<h3>5.2 回归算法流程</h3>\n<pre><code class=\"language-python\">from sklearn.neighbors import KNeighborsRegressor\n\n# 数据切分和标准化（与分类相同）\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0)\nX_train_scaled = (X_train - mu) / sigma\nX_test_scaled = (X_test - mu) / sigma\n\n# 回归模型\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X_train_scaled, y_train)\ny_pred = knn_reg.predict(X_test_scaled)\n</code></pre>\n<h3>5.3 回归评估指标</h3>\n<p><strong>平均绝对误差 (MAE)</strong>：MAE = (1/n) × Σᵢ₌₁ⁿ |yᵢ - ŷᵢ|</p>\n<p><strong>均方误差 (MSE)</strong>：MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²</p>\n<pre><code class=\"language-python\"># 评估指标计算\nmae = abs(y_pred - y_test).mean()\nmse = ((y_pred - y_test) ** 2).mean()\nrmse = mse ** 0.5\n\nprint(f\"平均绝对误差: {mae:.4f}\")\nprint(f\"均方误差: {mse:.4f}\")\nprint(f\"均方根误差: {rmse:.4f}\")\n</code></pre>\n<hr>\n<h2>🔑 核心要点总结</h2>\n<h3>NumPy 核心概念</h3>\n<ul>\n<li><strong>向量化操作</strong>：提高计算效率的关键</li>\n<li><strong>广播机制</strong>：实现不同形状数组的运算</li>\n<li><strong>线性代数函数</strong>：模长、内积、距离计算</li>\n</ul>\n<h3>特征工程要点</h3>\n<ul>\n<li><strong>标准化必要性</strong>：消除特征间的量纲差异</li>\n<li><strong>数据泄露防范</strong>：始终用训练集统计量处理测试集</li>\n<li><strong>Z-score vs Min-Max</strong>：根据数据分布选择合适方法</li>\n</ul>\n<h3>KNN 算法核心</h3>\n<ul>\n<li><strong>距离度量</strong>：欧几里得距离最常用</li>\n<li><strong>K 值选择</strong>：影响模型复杂度和性能</li>\n<li><strong>分类 vs 回归</strong>：投票机制 vs 平均值机制</li>\n</ul>\n<h3>机器学习标准流程</h3>\n<ol>\n<li><strong>数据加载</strong> → 2. <strong>数据切分</strong> → 3. <strong>特征预处理</strong> → 4. <strong>模型训练</strong> → 5. <strong>预测评估</strong></li>\n</ol>\n<hr>\n<h2>🚀 实践建议</h2>\n<ol>\n<li><strong>代码实践</strong>：每个概念都要亲自编写代码验证</li>\n<li><strong>参数调优</strong>：尝试不同的 K 值，观察性能变化</li>\n<li><strong>数据可视化</strong>：用图表理解数据分布和算法效果</li>\n<li><strong>扩展学习</strong>：了解其他距离度量方法和预处理技术</li>\n</ol>\n<hr>\n<h2>学习心得</h2>\n<p>这节课系统地学习了机器学习的基础工具和算法：</p>\n<ol>\n<li><strong>NumPy的重要性</strong>：作为科学计算基础，向量化操作大大提升了计算效率</li>\n<li><strong>特征工程的关键性</strong>：数据预处理直接影响模型性能，标准化是必不可少的步骤</li>\n<li><strong>KNN算法的简洁性</strong>：虽然简单，但在很多场景下都很有效</li>\n<li><strong>机器学习流程的规范性</strong>：严格按照标准流程可以避免很多常见错误</li>\n</ol>\n<h2>下一步学习计划</h2>\n<ul>\n<li>06-17</li>\n</ul>"
    },
    "_id": "learning-notes/2024-06-14-getting-started.md",
    "_raw": {
      "sourceFilePath": "learning-notes/2024-06-14-getting-started.md",
      "sourceFileName": "2024-06-14-getting-started.md",
      "sourceFileDir": "learning-notes",
      "contentType": "markdown",
      "flattenedPath": "learning-notes/2024-06-14-getting-started"
    },
    "type": "LearningNote",
    "url": "/learning-notes/2024-06-14-getting-started"
  },
  {
    "title": "机器学习03 - AIE54 Day03学习笔记",
    "date": "2024-06-17T00:00:00.000Z",
    "summary": "机器学习第三天学习内容，包括数据预处理、线性回归、逻辑回归、模型评估和模型保存",
    "tags": [
      "机器学习",
      "线性回归",
      "逻辑回归",
      "数据预处理",
      "模型评估",
      "AIE54"
    ],
    "readingTime": 18,
    "hasImages": true,
    "slug": "2024-06-17",
    "body": {
      "raw": "\n# 机器学习03 - AIE54 Day03学习笔记\n\n> **学习日期**：2024-06-17  \n> **主讲老师**：李晓华  \n> **课时**：2  \n> **文档来源**：day03-机器学习03.pdf\n\n## 课程关键词\n机器学习 | 数据预处理 | 线性回归 | 逻辑回归 | 模型评估 | KNN算法\n\n---\n\n## 🎯 第一部分：机器学习基础概念\n\n### 1.1 有监督学习流程\n\n**标准机器学习流程**包括以下关键步骤：\n\n1. **数据收集与加载**\n2. **特征工程与数据预处理**\n3. **模型选择与训练**\n4. **模型评估与优化**\n5. **模型部署与保存**\n\n### 1.2 回归 vs 分类\n\n**核心区别**：\n\n- **回归问题**：预测连续数值（如房价预测、温度预测）\n- **分类问题**：预测离散类别（如疾病诊断、图像识别）\n\n**应用场景**：\n- 回归：股价预测、销量预测、年龄估计\n- 分类：垃圾邮件检测、情感分析、医疗诊断\n\n---\n\n## 🔧 第二部分：数据预处理与标准化\n\n### 2.1 数据标准化的重要性\n\n**核心目的**：消除不同特征之间量纲差异的影响，确保所有特征在相同尺度上参与模型训练。\n\n**为什么需要标准化？**\n- 不同特征的数值范围可能相差巨大\n- 大数值特征会主导模型学习过程\n- 影响基于距离的算法（如KNN、SVM）\n\n### 2.2 标准化公式\n\n**Z-score 标准化公式**：\n\nX_normalized = (X - μ) / (σ + ε)\n\n其中：\n- μ 是均值\n- σ 是标准差  \n- ε = 1e-9 是防止除零的小常数\n\n### 2.3 代码实现\n\n```python\n# 计算训练集的均值和标准差\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\n\n# 对训练集和测试集进行标准化\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n```\n\n**⚠️ 重要注意事项**：\n- 测试集必须使用训练集的均值和标准差进行标准化\n- 避免数据泄露问题\n- 保证模型在真实环境中的泛化能力\n\n---\n\n## 📈 第三部分：线性回归\n\n### 3.1 理论基础\n\n**线性回归假设**：目标变量与特征变量之间存在线性关系\n\n**数学表达式**：\ny = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ + ε\n\n其中：\n- y 是目标变量\n- wᵢ 是权重参数\n- xᵢ 是特征变量\n- ε 是误差项\n\n### 3.2 损失函数\n\n**均方误差（MSE）**作为损失函数：\n\nMSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²\n\n**特点**：\n- 对异常值敏感\n- 可导，便于优化\n- 几何意义明确\n\n### 3.3 完整代码示例\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# 1. 数据加载\ndata = pd.read_csv(\"boston_house_prices.csv\", skiprows=1)\n\n# 2. 特征和标签分离\nX = data.loc[:, :\"LSTAT\"].to_numpy()  # 特征\ny = data.loc[:, \"MEDV\"].to_numpy()    # 标签\n\n# 3. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n# 4. 数据标准化\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n\n# 5. 模型训练\nlr = LinearRegression()\nlr.fit(X=X_train, y=y_train)\n\n# 6. 预测与评估\ny_pred = lr.predict(X=X_test)\nmse = ((y_pred - y_test) ** 2).mean()\nprint(f\"均方误差: {mse}\")\n```\n\n---\n\n## 🧠 第四部分：逻辑回归与分类\n\n### 4.1 Sigmoid 激活函数\n\n**核心作用**：将线性输出映射到 (0,1) 区间，表示概率\n\n**数学公式**：\nσ(z) = 1 / (1 + e^(-z))\n\n### 4.2 Sigmoid 函数特性\n\n**重要特性**：\n- **输出范围**：(0, 1)\n- **S型曲线**：平滑的概率转换\n- **导数易计算**：σ'(z) = σ(z)(1 - σ(z))\n- **单调递增**：适合概率建模\n\n### 4.3 代码实现\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    \"\"\"Sigmoid 激活函数\"\"\"\n    return 1 / (1 + np.exp(-x))\n\n# 可视化 Sigmoid 函数\nx = np.linspace(-10, 10, 100)\nplt.plot(x, sigmoid(x))\nplt.grid()\nplt.title('Sigmoid Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.show()\n```\n\n### 4.4 逻辑回归完整流程\n\n```python\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# 1. 加载数据\nX, y = load_breast_cancer(return_X_y=True)\n\n# 2. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n# 3. 数据标准化\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n\n# 4. 模型训练\nlr = LogisticRegression()\nlr.fit(X=X_train, y=y_train)\n\n# 5. 预测与评估\ny_pred = lr.predict(X=X_test)\naccuracy = (y_pred == y_test).mean()\nprint(f\"准确率: {accuracy:.4f}\")\n\n# 查看模型参数\nprint(f\"权重数量: {lr.coef_.shape}\")\nprint(f\"偏置项: {lr.intercept_}\")\n```\n\n---\n\n## 📊 第五部分：模型评估\n\n### 5.1 回归问题评估指标\n\n**均方误差 (MSE)**：\n\nMSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²\n\n```python\nmse = ((y_pred - y_test) ** 2).mean()\n```\n\n**其他常用指标**：\n- **MAE**：平均绝对误差\n- **RMSE**：均方根误差\n- **R²**：决定系数\n\n### 5.2 分类问题评估指标\n\n**准确率 (Accuracy)**：\n\nAccuracy = 正确预测数量 / 总预测数量\n\n```python\naccuracy = (y_pred == y_test).mean()\n```\n\n**其他重要指标**：\n- **精确率 (Precision)**\n- **召回率 (Recall)**\n- **F1-Score**\n- **AUC-ROC**\n\n---\n\n## 💾 第六部分：模型保存与加载\n\n### 6.1 序列化概念\n\n**核心概念**：\n- **序列化**：将内存中的对象转化为字节流，保存到硬盘\n- **反序列化**：将硬盘上的文件读入，转化为内存中的对象\n\n### 6.2 Python 序列化工具对比\n\n| 工具 | 特点 | 适用场景 |\n|------|------|----------|\n| **pickle** | 底层，操作相对复杂 | 通用Python对象序列化 |\n| **joblib** | 上层，操作简便 | 专为大型NumPy数组和科学计算优化 |\n\n### 6.3 代码示例\n\n**使用 pickle 保存单个模型**：\n\n```python\nimport pickle\n\n# 保存模型\nwith open(\"lr.pickle\", \"wb\") as f:\n    pickle.dump(obj=lr, file=f)\n\n# 加载模型\nwith open(\"lr.pickle\", \"rb\") as f:\n    lr_loaded = pickle.load(file=f)\n```\n\n**使用 joblib 保存多个模型**：\n\n```python\nimport joblib\n\n# 保存多个模型\njoblib.dump(value=knn, filename=\"knn.joblib\")\njoblib.dump(value=[lr, knn], filename=\"models.joblib\")\n\n# 加载模型\nknn_loaded = joblib.load(filename=\"knn.joblib\")\nlr_loaded, knn_loaded = joblib.load(filename=\"models.joblib\")\n```\n\n---\n\n## 📐 第七部分：统计学概念：方差\n\n### 7.1 方差的两种估计方法\n\n**1. 总体方差（有偏估计）**：\n\nσ² = (1/N) × Σᵢ₌₁ᴺ (xᵢ - μ)²\n\n**2. 样本方差（无偏估计）**：\n\ns² = (1/(N-1)) × Σᵢ₌₁ᴺ (xᵢ - x̄)²\n\n### 7.2 库函数差异\n\n| 库 | 默认方法 | 参数控制 |\n|-----|----------|----------|\n| **NumPy** | 有偏估计 (除以N) | ddof=1 使用无偏估计 |\n| **PyTorch** | 无偏估计 (除以N-1) | correction=1 控制 |\n\n### 7.3 代码示例\n\n```python\nimport numpy as np\nimport torch\n\nls = [1, 2, 3, 4, 5, 6]\n\n# NumPy - 默认有偏估计\narr = np.array(ls)\nbiased_std = arr.std()        # 有偏\nunbiased_std = arr.std(ddof=1)  # 无偏\n\n# PyTorch - 默认无偏估计\nt = torch.tensor(ls, dtype=torch.float32)\nunbiased_std = t.std(correction=1)  # 无偏\n```\n\n### 7.4 注意事项\n\n1. **防止除零错误**：\n```python\nsigma = X_train.std(axis=0) + 1e-9\n```\n\n2. **选择合适的估计方法**：\n   - 样本量较小时，使用无偏估计更准确\n   - 深度学习中，通常使用有偏估计（计算效率更高）\n\n---\n\n## 🚀 第八部分：KNN算法补充\n\n### 8.1 K近邻算法特点\n\n**核心特性**：\n- **非参数方法**：不需要假设数据分布\n- **懒惰学习**：训练阶段只存储数据，预测时才计算\n- **适用性广**：回归和分类问题都适用\n\n### 8.2 代码示例\n\n```python\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n\n# 回归任务\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X=X_train, y=y_train)\n\n# 分类任务\nknn_clf = KNeighborsClassifier(n_neighbors=5)\nknn_clf.fit(X=X_train, y=y_train)\n```\n\n---\n\n## 🎯 关键要点总结\n\n### ✅ 最佳实践\n\n1. **数据预处理必不可少**：标准化能显著提升模型性能\n2. **合理划分数据集**：训练集用于学习，测试集用于评估\n3. **选择合适的评估指标**：MSE用于回归，准确率用于分类\n4. **模型保存很重要**：使用joblib保存训练好的模型\n\n### 🔍 深入理解\n\n1. **线性回归**：适用于线性关系明显的回归问题\n2. **逻辑回归**：通过Sigmoid函数实现分类，本质上是线性分类器\n3. **方差估计**：理解有偏与无偏的区别，根据场景选择\n\n### 📈 进阶方向\n\n- **正则化方法**（Ridge、Lasso）\n- **模型选择与交叉验证**\n- **特征工程技巧**\n- **集成学习方法**\n\n---\n\n## 学习心得\n\n这节课深入学习了机器学习的核心算法和实践技巧：\n\n1. **数据预处理的重要性**：标准化不仅是技术要求，更是保证模型公平性的关键步骤\n2. **线性模型的威力**：虽然简单，但线性回归和逻辑回归在很多实际问题中都表现优异\n3. **模型评估的科学性**：选择合适的评估指标对模型性能判断至关重要\n4. **工程实践的价值**：模型保存、加载等工程技巧是实际部署的必备技能\n\n## 下一步学习计划\n- 深入学习正则化技术\n- 探索更复杂的非线性模型\n- 实践特征工程技巧\n- 学习模型调优方法\n",
      "html": "<h1>机器学习03 - AIE54 Day03学习笔记</h1>\n<blockquote>\n<p><strong>学习日期</strong>：2024-06-17<br>\n<strong>主讲老师</strong>：李晓华<br>\n<strong>课时</strong>：2<br>\n<strong>文档来源</strong>：day03-机器学习03.pdf</p>\n</blockquote>\n<h2>课程关键词</h2>\n<p>机器学习 | 数据预处理 | 线性回归 | 逻辑回归 | 模型评估 | KNN算法</p>\n<hr>\n<h2>🎯 第一部分：机器学习基础概念</h2>\n<h3>1.1 有监督学习流程</h3>\n<p><strong>标准机器学习流程</strong>包括以下关键步骤：</p>\n<ol>\n<li><strong>数据收集与加载</strong></li>\n<li><strong>特征工程与数据预处理</strong></li>\n<li><strong>模型选择与训练</strong></li>\n<li><strong>模型评估与优化</strong></li>\n<li><strong>模型部署与保存</strong></li>\n</ol>\n<h3>1.2 回归 vs 分类</h3>\n<p><strong>核心区别</strong>：</p>\n<ul>\n<li><strong>回归问题</strong>：预测连续数值（如房价预测、温度预测）</li>\n<li><strong>分类问题</strong>：预测离散类别（如疾病诊断、图像识别）</li>\n</ul>\n<p><strong>应用场景</strong>：</p>\n<ul>\n<li>回归：股价预测、销量预测、年龄估计</li>\n<li>分类：垃圾邮件检测、情感分析、医疗诊断</li>\n</ul>\n<hr>\n<h2>🔧 第二部分：数据预处理与标准化</h2>\n<h3>2.1 数据标准化的重要性</h3>\n<p><strong>核心目的</strong>：消除不同特征之间量纲差异的影响，确保所有特征在相同尺度上参与模型训练。</p>\n<p><strong>为什么需要标准化？</strong></p>\n<ul>\n<li>不同特征的数值范围可能相差巨大</li>\n<li>大数值特征会主导模型学习过程</li>\n<li>影响基于距离的算法（如KNN、SVM）</li>\n</ul>\n<h3>2.2 标准化公式</h3>\n<p><strong>Z-score 标准化公式</strong>：</p>\n<p>X_normalized = (X - μ) / (σ + ε)</p>\n<p>其中：</p>\n<ul>\n<li>μ 是均值</li>\n<li>σ 是标准差</li>\n<li>ε = 1e-9 是防止除零的小常数</li>\n</ul>\n<h3>2.3 代码实现</h3>\n<pre><code class=\"language-python\"># 计算训练集的均值和标准差\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\n\n# 对训练集和测试集进行标准化\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n</code></pre>\n<p><strong>⚠️ 重要注意事项</strong>：</p>\n<ul>\n<li>测试集必须使用训练集的均值和标准差进行标准化</li>\n<li>避免数据泄露问题</li>\n<li>保证模型在真实环境中的泛化能力</li>\n</ul>\n<hr>\n<h2>📈 第三部分：线性回归</h2>\n<h3>3.1 理论基础</h3>\n<p><strong>线性回归假设</strong>：目标变量与特征变量之间存在线性关系</p>\n<p><strong>数学表达式</strong>：\ny = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ + ε</p>\n<p>其中：</p>\n<ul>\n<li>y 是目标变量</li>\n<li>wᵢ 是权重参数</li>\n<li>xᵢ 是特征变量</li>\n<li>ε 是误差项</li>\n</ul>\n<h3>3.2 损失函数</h3>\n<p>**均方误差（MSE）**作为损失函数：</p>\n<p>MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²</p>\n<p><strong>特点</strong>：</p>\n<ul>\n<li>对异常值敏感</li>\n<li>可导，便于优化</li>\n<li>几何意义明确</li>\n</ul>\n<h3>3.3 完整代码示例</h3>\n<pre><code class=\"language-python\">import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# 1. 数据加载\ndata = pd.read_csv(\"boston_house_prices.csv\", skiprows=1)\n\n# 2. 特征和标签分离\nX = data.loc[:, :\"LSTAT\"].to_numpy()  # 特征\ny = data.loc[:, \"MEDV\"].to_numpy()    # 标签\n\n# 3. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n# 4. 数据标准化\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n\n# 5. 模型训练\nlr = LinearRegression()\nlr.fit(X=X_train, y=y_train)\n\n# 6. 预测与评估\ny_pred = lr.predict(X=X_test)\nmse = ((y_pred - y_test) ** 2).mean()\nprint(f\"均方误差: {mse}\")\n</code></pre>\n<hr>\n<h2>🧠 第四部分：逻辑回归与分类</h2>\n<h3>4.1 Sigmoid 激活函数</h3>\n<p><strong>核心作用</strong>：将线性输出映射到 (0,1) 区间，表示概率</p>\n<p><strong>数学公式</strong>：\nσ(z) = 1 / (1 + e^(-z))</p>\n<h3>4.2 Sigmoid 函数特性</h3>\n<p><strong>重要特性</strong>：</p>\n<ul>\n<li><strong>输出范围</strong>：(0, 1)</li>\n<li><strong>S型曲线</strong>：平滑的概率转换</li>\n<li><strong>导数易计算</strong>：σ'(z) = σ(z)(1 - σ(z))</li>\n<li><strong>单调递增</strong>：适合概率建模</li>\n</ul>\n<h3>4.3 代码实现</h3>\n<pre><code class=\"language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    \"\"\"Sigmoid 激活函数\"\"\"\n    return 1 / (1 + np.exp(-x))\n\n# 可视化 Sigmoid 函数\nx = np.linspace(-10, 10, 100)\nplt.plot(x, sigmoid(x))\nplt.grid()\nplt.title('Sigmoid Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.show()\n</code></pre>\n<h3>4.4 逻辑回归完整流程</h3>\n<pre><code class=\"language-python\">from sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# 1. 加载数据\nX, y = load_breast_cancer(return_X_y=True)\n\n# 2. 数据切分\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n# 3. 数据标准化\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-9\nX_train = (X_train - mu) / sigma\nX_test = (X_test - mu) / sigma\n\n# 4. 模型训练\nlr = LogisticRegression()\nlr.fit(X=X_train, y=y_train)\n\n# 5. 预测与评估\ny_pred = lr.predict(X=X_test)\naccuracy = (y_pred == y_test).mean()\nprint(f\"准确率: {accuracy:.4f}\")\n\n# 查看模型参数\nprint(f\"权重数量: {lr.coef_.shape}\")\nprint(f\"偏置项: {lr.intercept_}\")\n</code></pre>\n<hr>\n<h2>📊 第五部分：模型评估</h2>\n<h3>5.1 回归问题评估指标</h3>\n<p><strong>均方误差 (MSE)</strong>：</p>\n<p>MSE = (1/n) × Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²</p>\n<pre><code class=\"language-python\">mse = ((y_pred - y_test) ** 2).mean()\n</code></pre>\n<p><strong>其他常用指标</strong>：</p>\n<ul>\n<li><strong>MAE</strong>：平均绝对误差</li>\n<li><strong>RMSE</strong>：均方根误差</li>\n<li><strong>R²</strong>：决定系数</li>\n</ul>\n<h3>5.2 分类问题评估指标</h3>\n<p><strong>准确率 (Accuracy)</strong>：</p>\n<p>Accuracy = 正确预测数量 / 总预测数量</p>\n<pre><code class=\"language-python\">accuracy = (y_pred == y_test).mean()\n</code></pre>\n<p><strong>其他重要指标</strong>：</p>\n<ul>\n<li><strong>精确率 (Precision)</strong></li>\n<li><strong>召回率 (Recall)</strong></li>\n<li><strong>F1-Score</strong></li>\n<li><strong>AUC-ROC</strong></li>\n</ul>\n<hr>\n<h2>💾 第六部分：模型保存与加载</h2>\n<h3>6.1 序列化概念</h3>\n<p><strong>核心概念</strong>：</p>\n<ul>\n<li><strong>序列化</strong>：将内存中的对象转化为字节流，保存到硬盘</li>\n<li><strong>反序列化</strong>：将硬盘上的文件读入，转化为内存中的对象</li>\n</ul>\n<h3>6.2 Python 序列化工具对比</h3>\n<p>| 工具 | 特点 | 适用场景 |\n|------|------|----------|\n| <strong>pickle</strong> | 底层，操作相对复杂 | 通用Python对象序列化 |\n| <strong>joblib</strong> | 上层，操作简便 | 专为大型NumPy数组和科学计算优化 |</p>\n<h3>6.3 代码示例</h3>\n<p><strong>使用 pickle 保存单个模型</strong>：</p>\n<pre><code class=\"language-python\">import pickle\n\n# 保存模型\nwith open(\"lr.pickle\", \"wb\") as f:\n    pickle.dump(obj=lr, file=f)\n\n# 加载模型\nwith open(\"lr.pickle\", \"rb\") as f:\n    lr_loaded = pickle.load(file=f)\n</code></pre>\n<p><strong>使用 joblib 保存多个模型</strong>：</p>\n<pre><code class=\"language-python\">import joblib\n\n# 保存多个模型\njoblib.dump(value=knn, filename=\"knn.joblib\")\njoblib.dump(value=[lr, knn], filename=\"models.joblib\")\n\n# 加载模型\nknn_loaded = joblib.load(filename=\"knn.joblib\")\nlr_loaded, knn_loaded = joblib.load(filename=\"models.joblib\")\n</code></pre>\n<hr>\n<h2>📐 第七部分：统计学概念：方差</h2>\n<h3>7.1 方差的两种估计方法</h3>\n<p><strong>1. 总体方差（有偏估计）</strong>：</p>\n<p>σ² = (1/N) × Σᵢ₌₁ᴺ (xᵢ - μ)²</p>\n<p><strong>2. 样本方差（无偏估计）</strong>：</p>\n<p>s² = (1/(N-1)) × Σᵢ₌₁ᴺ (xᵢ - x̄)²</p>\n<h3>7.2 库函数差异</h3>\n<p>| 库 | 默认方法 | 参数控制 |\n|-----|----------|----------|\n| <strong>NumPy</strong> | 有偏估计 (除以N) | ddof=1 使用无偏估计 |\n| <strong>PyTorch</strong> | 无偏估计 (除以N-1) | correction=1 控制 |</p>\n<h3>7.3 代码示例</h3>\n<pre><code class=\"language-python\">import numpy as np\nimport torch\n\nls = [1, 2, 3, 4, 5, 6]\n\n# NumPy - 默认有偏估计\narr = np.array(ls)\nbiased_std = arr.std()        # 有偏\nunbiased_std = arr.std(ddof=1)  # 无偏\n\n# PyTorch - 默认无偏估计\nt = torch.tensor(ls, dtype=torch.float32)\nunbiased_std = t.std(correction=1)  # 无偏\n</code></pre>\n<h3>7.4 注意事项</h3>\n<ol>\n<li><strong>防止除零错误</strong>：</li>\n</ol>\n<pre><code class=\"language-python\">sigma = X_train.std(axis=0) + 1e-9\n</code></pre>\n<ol start=\"2\">\n<li><strong>选择合适的估计方法</strong>：\n<ul>\n<li>样本量较小时，使用无偏估计更准确</li>\n<li>深度学习中，通常使用有偏估计（计算效率更高）</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2>🚀 第八部分：KNN算法补充</h2>\n<h3>8.1 K近邻算法特点</h3>\n<p><strong>核心特性</strong>：</p>\n<ul>\n<li><strong>非参数方法</strong>：不需要假设数据分布</li>\n<li><strong>懒惰学习</strong>：训练阶段只存储数据，预测时才计算</li>\n<li><strong>适用性广</strong>：回归和分类问题都适用</li>\n</ul>\n<h3>8.2 代码示例</h3>\n<pre><code class=\"language-python\">from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n\n# 回归任务\nknn_reg = KNeighborsRegressor(n_neighbors=5)\nknn_reg.fit(X=X_train, y=y_train)\n\n# 分类任务\nknn_clf = KNeighborsClassifier(n_neighbors=5)\nknn_clf.fit(X=X_train, y=y_train)\n</code></pre>\n<hr>\n<h2>🎯 关键要点总结</h2>\n<h3>✅ 最佳实践</h3>\n<ol>\n<li><strong>数据预处理必不可少</strong>：标准化能显著提升模型性能</li>\n<li><strong>合理划分数据集</strong>：训练集用于学习，测试集用于评估</li>\n<li><strong>选择合适的评估指标</strong>：MSE用于回归，准确率用于分类</li>\n<li><strong>模型保存很重要</strong>：使用joblib保存训练好的模型</li>\n</ol>\n<h3>🔍 深入理解</h3>\n<ol>\n<li><strong>线性回归</strong>：适用于线性关系明显的回归问题</li>\n<li><strong>逻辑回归</strong>：通过Sigmoid函数实现分类，本质上是线性分类器</li>\n<li><strong>方差估计</strong>：理解有偏与无偏的区别，根据场景选择</li>\n</ol>\n<h3>📈 进阶方向</h3>\n<ul>\n<li><strong>正则化方法</strong>（Ridge、Lasso）</li>\n<li><strong>模型选择与交叉验证</strong></li>\n<li><strong>特征工程技巧</strong></li>\n<li><strong>集成学习方法</strong></li>\n</ul>\n<hr>\n<h2>学习心得</h2>\n<p>这节课深入学习了机器学习的核心算法和实践技巧：</p>\n<ol>\n<li><strong>数据预处理的重要性</strong>：标准化不仅是技术要求，更是保证模型公平性的关键步骤</li>\n<li><strong>线性模型的威力</strong>：虽然简单，但线性回归和逻辑回归在很多实际问题中都表现优异</li>\n<li><strong>模型评估的科学性</strong>：选择合适的评估指标对模型性能判断至关重要</li>\n<li><strong>工程实践的价值</strong>：模型保存、加载等工程技巧是实际部署的必备技能</li>\n</ol>\n<h2>下一步学习计划</h2>\n<ul>\n<li>深入学习正则化技术</li>\n<li>探索更复杂的非线性模型</li>\n<li>实践特征工程技巧</li>\n<li>学习模型调优方法</li>\n</ul>"
    },
    "_id": "learning-notes/2024-06-17.md",
    "_raw": {
      "sourceFilePath": "learning-notes/2024-06-17.md",
      "sourceFileName": "2024-06-17.md",
      "sourceFileDir": "learning-notes",
      "contentType": "markdown",
      "flattenedPath": "learning-notes/2024-06-17"
    },
    "type": "LearningNote",
    "url": "/learning-notes/2024-06-17"
  }
]